<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>vLLM引擎解析-调度分析 | WBINGのBLOG</title><meta name="author" content="weibingo"><meta name="copyright" content="weibingo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一、vLLM 的调用方式根据 vLLM 的官方文档，它向用户提供了两种调用它的方法，分别是：  Offline Batched Inference （ 同步 ，离线批处理）  API Server For Online Serving （ 异步 ，在线推理服务），在这下面又提供了 2 种支持的 API 类型：  OpenAI-Compatible API Server （官方推荐）：兼容了 Ope">
<meta property="og:type" content="article">
<meta property="og:title" content="vLLM引擎解析-调度分析">
<meta property="og:url" content="https://wbice.cn/article/vLLM%E5%BC%95%E6%93%8E%E8%A7%A3%E6%9E%90-%E8%B0%83%E5%BA%A6%E5%88%86%E6%9E%90.html">
<meta property="og:site_name" content="WBINGのBLOG">
<meta property="og:description" content="一、vLLM 的调用方式根据 vLLM 的官方文档，它向用户提供了两种调用它的方法，分别是：  Offline Batched Inference （ 同步 ，离线批处理）  API Server For Online Serving （ 异步 ，在线推理服务），在这下面又提供了 2 种支持的 API 类型：  OpenAI-Compatible API Server （官方推荐）：兼容了 Ope">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hexo-1256892004.cos.ap-beijing.myqcloud.com/page/avatar.jpg">
<meta property="article:published_time" content="2024-08-25T11:13:30.000Z">
<meta property="article:modified_time" content="2024-09-07T02:53:12.736Z">
<meta property="article:author" content="weibingo">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hexo-1256892004.cos.ap-beijing.myqcloud.com/page/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://wbice.cn/article/vLLM%E5%BC%95%E6%93%8E%E8%A7%A3%E6%9E%90-%E8%B0%83%E5%BA%A6%E5%88%86%E6%9E%90.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"L2E2THKGS7","apiKey":"484cae4b356f0dd9161e6eff1bff38f1","indexName":"prod_weibing-blog","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'vLLM引擎解析-调度分析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-07 10:53:12'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://hexo-1256892004.cos.ap-beijing.myqcloud.com/page/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">55</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 目录</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="WBINGのBLOG"><span class="site-name">WBINGのBLOG</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 目录</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">vLLM引擎解析-调度分析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-25T11:13:30.000Z" title="发表于 2024-08-25 19:13:30">2024-08-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-07T02:53:12.736Z" title="更新于 2024-09-07 10:53:12">2024-09-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%BC%95%E6%93%8E/">引擎</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">14.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>52分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="vLLM引擎解析-调度分析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="一、vLLM-的调用方式"><a href="#一、vLLM-的调用方式" class="headerlink" title="一、vLLM 的调用方式"></a>一、vLLM 的调用方式</h2><p>根据 vLLM 的<a target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html">官方文档</a>，它向用户提供了两种调用它的方法，分别是：</p>
<ul>
<li><p><strong>Offline Batched Inference</strong> （ <strong>同步</strong> ，离线批处理）</p>
</li>
<li><p><strong>API Server For Online Serving</strong> （ <strong>异步</strong> ，在线推理服务），在这下面又提供了 2 种支持的 API 类型：</p>
<ul>
<li><strong>OpenAI-Compatible API Server</strong> （官方推荐）：兼容了 OpenAI 请求格式的 server，包括 OpenAI Completions API 和 OpenAI Chat API。</li>
<li><strong>Simple Demo API Server</strong> （测试开发用，官方不推荐，相关脚本也不再维护）</li>
</ul>
</li>
</ul>
<p> <strong>在代码实现上，vLLM首先实现了一个推理内核引擎(LLMEngine)，在此基础上封装了上述两种调用方法</strong> 。在本系列的讲解中，我们会先以“offline bacthed inference”作为入口，详细解说内核引擎 LLMEngine 的各块细节。在此基础上我们再来看“online serving”的运作流程。</p>
<p>现在，让我们来看这两种调用方法的具体例子。</p>
<h3 id="1-1-Offline-Batched-Inference"><a href="#1-1-Offline-Batched-Inference" class="headerlink" title="1.1 Offline Batched Inference"></a>1.1 Offline Batched Inference</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">from vllm import LLM, SamplingParams</span><br><span class="line"></span><br><span class="line"># ===========================================================================</span><br><span class="line"># batch prompts</span><br><span class="line"># ===========================================================================</span><br><span class="line">prompts = [&quot;Hello, my name is&quot;,</span><br><span class="line">           &quot;The president of the United States is&quot;,</span><br><span class="line">           &quot;The capital of France is&quot;,</span><br><span class="line">           &quot;The future of AI is&quot;,]</span><br><span class="line"></span><br><span class="line"># ===========================================================================</span><br><span class="line"># 采样参数</span><br><span class="line"># ===========================================================================</span><br><span class="line">sampling_params = SamplingParams(temperature=0.8, top_p=0.95)</span><br><span class="line"></span><br><span class="line"># ===========================================================================</span><br><span class="line"># 初始化vLLM offline batched inference实例，并加载指定模型</span><br><span class="line"># ===========================================================================</span><br><span class="line">llm = LLM(model=&quot;facebook/opt-125m&quot;)</span><br><span class="line"></span><br><span class="line"># ===========================================================================</span><br><span class="line"># 推理</span><br><span class="line"># ===========================================================================</span><br><span class="line">outputs = llm.generate(prompts, sampling_params)</span><br><span class="line"></span><br><span class="line"># ===========================================================================</span><br><span class="line"># 对每一条prompt，打印其推理结果</span><br><span class="line"># ===========================================================================</span><br><span class="line">for output in outputs:</span><br><span class="line">    prompt = output.prompt</span><br><span class="line">    generated_text = output.outputs[0].text</span><br><span class="line">    print(f&quot;Prompt: &#123;prompt!r&#125;, Generated text: &#123;generated_text!r&#125;&quot;)</span><br></pre></td></tr></table></figure>

<p>在传统离线批处理中，我们每次给模型发送推理请求时，都要：</p>
<ul>
<li>等一个 batch 的数据齐全后，一起发送</li>
<li>整个 batch 的数据一起做推理</li>
<li>等一个 batch 的数据全部推理完毕后，一起返回推理结果</li>
</ul>
<p> <strong>这种“团体间等成员到齐，再一起行动”的行为，就被称为“同步”</strong> 。</p>
<p>在 vLLM 中，当我们使用离线批处理模式时，表面上是在做“同步”推理，也即 batch_size 是静态固定的。 <strong>但推理内核引擎（LLMEngine）在实际运作时，batch_size是可以动态变更的</strong> ：在每一个推理阶段（ <strong>prefill算1个推理阶段，每个decode各算1个推理阶段</strong> ）处理的 batch size 可以根据当下显存的实际使用情况而变动。</p>
<p>举个例子来说：</p>
<ul>
<li>给定一个很大的 batch，此时尽管 vLLM 采用了 PagedAttention 这样的显存优化技术，我们的 gpu 依然无法同时处理这么大的 batch。</li>
<li>所以 batch 中的每一条数据，会被先放到一个 waiting 队列中。vLLM 会用自己的调度策略从 waiting 队列中依次取数，加入 running 队列中，直到它认为取出的这些数据将会打满它为 1 个推理阶段分配好的显存。此时 waiting 队列中可能还会剩一些数据。</li>
<li>在每 1 个推理阶段，vLLM 对 running 队列中的数据做推理。如果这 1 个推理阶段执行完毕后，有的数据已经完成了生成（比如正常遇到 <code>&lt;eos&gt;</code> 了），就将这些完成的数据从 running 队列中移开，并释放它占据的物理块显存。</li>
<li>这时，waiting 队列中的数据就可以继续 append 进 running 队列中，做下 1 个阶段的推理。</li>
<li>因此在每 1 个推理阶段，vLLM 处理的 batch size 可能会动态变更。</li>
<li>将 LLMEngine 包装成离线批处理形式后，所有的数据必须等到一起做完推理才能返给我们。所以从体感上，我们可能很难感知到内核引擎的“动态”逻辑。</li>
</ul>
<p> <strong>以上是一个浅显粗暴的例子，目的是帮助大家理解“在vLLM中，即使是同步形式的离线批处理，其背后的内核引擎也是按动态batch的形式来实现的”</strong> ，实际的调度策略（Scheduler）要更加复杂，我们将在后续的解读中来具体看它。</p>
<p> <strong>也正是因为LLMEngine这种“动态处理”的特性，才使得它同时也能成为异步在线服务的内核引擎</strong> ：当一条条请求发来时，它们都先进入 LLMEngine 调度器（Scheduler）的 waiting 队列中（实际并不是直接进入 waiting 队列中的，而是在传给 LLMEngine 前先进入 asyncio.Queue()中，然后再由 LLMEngine 调度进 waiting 队列中的，这些细节我们也放在后面说，这里不影响理解就行）。此时模型正常执行它的 1 个推理阶段，调度器也正常处理新来的请求。当模型准备执行下 1 个推理阶段时，调度器再根据设定的策略，决定哪些数据可以进入 running 队列进行推理。由于在线服务是异步的，先推理完成的数据就可以先发给客户端了（如果采用流式传输，也可以生成多少先发多少）。<br><strong>在这个过程中，vLLM通过PagedAttention技术和“先来先服务（FCFS），后来先抢占，gpu不够就先swap到cpu上”的调度策略，在1个推理阶段处理尽可能多的请求，解决高并发场景下的推理吞吐问题。这就是整个vLLM运作的核心思想。</strong></p>
<p>‍</p>
<h3 id="1-2-API-Server-For-Online-Serving"><a href="#1-2-API-Server-For-Online-Serving" class="headerlink" title="1.2 API Server For Online Serving"></a>1.2 API Server For Online Serving</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># ===========================================================================</span><br><span class="line"># Server：起服务</span><br><span class="line"># ===========================================================================</span><br><span class="line">$ python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-7b-hf</span><br><span class="line"></span><br><span class="line"># ===========================================================================</span><br><span class="line"># Client：发请求（OpenAI API）</span><br><span class="line"># ===========================================================================</span><br><span class="line">$ curl http://localhost:8000/v1/completions \</span><br><span class="line">    -H &quot;Content-Type: application/json&quot; \</span><br><span class="line">    -d &#x27;&#123;</span><br><span class="line">        &quot;model&quot;: &quot;meta-llama/Llama-2-7b-hf&quot;,</span><br><span class="line">        &quot;prompt&quot;: &quot;San Francisco is a&quot;,</span><br><span class="line">        &quot;max_tokens&quot;: 7,</span><br><span class="line">        &quot;temperature&quot;: 0</span><br><span class="line">    &#125;&#x27;</span><br></pre></td></tr></table></figure>

<p>vLLM 在实现在线服务时，采用 uvicorn 部署 fastapi app 实例，以此实现异步的请求处理。而核心处理逻辑封装在 <code>AsyncLLMEngine</code> 类中（它继承自 LLMEngine）。<strong>所以，只要我们搞懂了LLMEngine，对vLLM的这两种调用方式就能举一反三了。</strong></p>
<h3 id="1-3-总结"><a href="#1-3-总结" class="headerlink" title="1.3 总结"></a>1.3 总结</h3><p>vLLM 的两种调用方式与内核引擎 LLMEngine 的关系如下（图片来自 vLLM 团队 2023 first meetup PPT）:​</p>
<p><img src="/assets/image-20240805144701-g417uor.png" alt="image">​</p>
<p><strong>图中左侧是用户使用界面，罗列了上述所说的两种调用方式</strong> （注意，如前文所说，做 demo 用的 api server 官方已经不再维护了，openai_api_server 才是官方推荐的使用方式，user custom server 目前还没有实现）。<strong>右侧则是开发者界面，不难发现LLMEngine是vLLM的核心逻辑。</strong></p>
<p>我们来看开发者界面下的几个函数，先来看 <strong>LLMEngine</strong> ：</p>
<ul>
<li><strong>add_request()</strong> ：该方法将每一个请求包装成 vLLM 能处理的数据类型(SequenceGroup，后面我们会详细解释)，并将其加入调度器（Scheduler）的 waiting 队列中。 <strong>在LLMEngine中，这个函数是按照“同步”的方式设计的</strong> ，也就是它被设计为“遍历 batch 中的每条数据，然后做相应处理”。所以这个函数本身只适合批处理场景。在异步的 online serving 中将会把它重写成异步的形式。</li>
<li><strong>abort_request</strong> ：在推理过程中，并不是所有的请求都能有返回结果。比如客户端断开连接时，这个请求的推理就可以终止了（abort），这个函数就被用来做这个操作。</li>
<li><strong>step()：负责执行1次推理过程（1个prefill算1个次推理，每个decode各算1次推理）</strong> 。在这个函数中，vLLM 的调度器会决定要送那些数据去执行本次推理，并负责给这些数据分配好物理块（这些信息都被作为 metadata 放在要送给模型做推理的数据中）。模型会根据这些信息，采用 PagedAttention 方法，实际完成推理。</li>
</ul>
<p><strong>AsyncLLMEngine</strong> 下的函数也是同理类推，这里不赘述了。</p>
<p>‍</p>
<h2 id="二、vLLM-代码整体架构"><a href="#二、vLLM-代码整体架构" class="headerlink" title="二、vLLM 代码整体架构"></a>二、vLLM 代码整体架构</h2><p><img src="/assets/image-20240805144722-r3jqm0j.png" alt="image">​</p>
<p>LLMEngine 可以具体分成两个部分：</p>
<h3 id="2-1-Centralized-Controller"><a href="#2-1-Centralized-Controller" class="headerlink" title="2.1 Centralized Controller"></a>2.1 Centralized Controller</h3><p> <strong>Centralized Controller，也就是前文我们所说的调度器(Scheduler)</strong> 。它和 LLMEngine 所在的进程是同一个，且两者都是在 CPU 上的。</p>
<ul>
<li><strong>调度器的主要作用就是，在每1个推理阶段，决定要把哪些数据送给模型做推理，同时负责给这些模型分配KV Cache物理块</strong> 。但要注意，它只是分配了物理块的 id，而不是物理块本身。物理块的实际分配是模型在推理过程中根据物理块 id 来操作的，也就是 CacheEngine 做的事情。</li>
<li><strong>调度器下维护着BlockSpaceManager。它负责管理BlockAllocator（实际参与分配物理块的类）。BlockAllocator又分成gpu和cpu两种类型，分别管理这两类设备上的物理块</strong> 。 <strong>你可能会问，cpu上的物理块是什么呢</strong> ？你还记得调度器有一个 swap 策略吗？当 gpu 上显存不足时，它会把后来的请求抢占，并将其相关的 KV cache 物理块全部都先 swap（置换、卸载）在 cpu 上，等后续 gpu 显存充足时，再把它们加载回 gpu 上继续做相关请求的推理。所以在 cpu 上我们也需要一个管控物理块的 BlockAllocator。</li>
</ul>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="2-2-Distributed-Workers"><a href="#2-2-Distributed-Workers" class="headerlink" title="2.2 Distributed Workers"></a>2.2 Distributed Workers</h3><p>Distributed Workers，也就是分布式系统。它的作用是将我们要使用的模型 load 到各块卡上（目前对单卡装不下的模型，vLLM 支持 tp&#x2F;pp 推理），然后对 Controller 传来的数据做 1 次推理，返回相关结果。我们来细看下这块：</p>
<ul>
<li><p><strong>Distributed Workers</strong> ：图中绘制为 Distributed Workers 这个绿色块， <strong>其实按vLLM的源码内容，写成Executor会更合适一些</strong> 。 <strong>它就是所有Workers的管控中心</strong> ，它指定了用什么方法管控这些 Workers，负责分布式环境的初始化，目前支持的方法有：</p>
<ul>
<li>cpu_executor：（较少用），使用 cpu 做推理时可考虑</li>
<li>gpu_executor：单卡（world_size &#x3D; 1）的情况下可用</li>
<li>ray_gpu_executor：使用 ray 这个分布式计算框架实现的 executor，适用于多卡环境</li>
<li>其他，如TPU</li>
</ul>
</li>
<li><p><strong>Worker</strong> ： <strong>在硬件上，它指gpu；在代码上，它指的是Worker实例（每个gpu上的进程维护自己的Worker实例）</strong> 。在每个 Worker 实例中又管控着如下两个重要实例：</p>
<ul>
<li><strong>CacheEngine：</strong> 负责管控 gpu&#x2F;cpu 上的 KV cache 物理块（调度器的 block manager 只负责物理块 id 的分配，CacheEngine 则是根据这个 id 分配结果实打实地在管理物理块中的数据）</li>
<li><strong>Worker.model</strong> ：根据 vLLM 代码，这里写成 <strong>model_runner</strong> 会更合适一些。 <strong>它负责加载模型，并执行推理</strong> 。PagedAttention 的相关逻辑，就维护这个实例关联的代码下。</li>
</ul>
</li>
</ul>
<h2 id="三、加载模型与预分配显存"><a href="#三、加载模型与预分配显存" class="headerlink" title="三、加载模型与预分配显存"></a>三、加载模型与预分配显存</h2><p>现在你已经从代码层面知道 vLLM 的整体架构了， <strong>你是不是迫不及待想看看：当一条请求过来时，整个vLLM是怎么运作的呢？</strong> 现在，我们就来解析这个流程。</p>
<p><strong>在vLLM正式开始处理1条请求（也就是LLMEngine的调度器正式开始运作时），它需要做两件和初始化相关的事：</strong></p>
<ul>
<li><strong>加载模型</strong></li>
<li><strong>预分配显存</strong></li>
</ul>
<p>我们分别来看这两个步骤。</p>
<h3 id="3-1-加载模型"><a href="#3-1-加载模型" class="headerlink" title="3.1 加载模型"></a>3.1 加载模型</h3><p><img src="/assets/image-20240805144734-crmywcx.png" alt="image">​</p>
<p>这里在做的事很直观：把你的 base model 加载到 worker 上。如果你是 online 加载的，vLLM 默认使用 HuggingFace，你也可以在环境变量中把相关配置改成 ModelScope。</p>
<h3 id="3-2-预分配显存"><a href="#3-2-预分配显存" class="headerlink" title="3.2 预分配显存"></a>3.2 预分配显存</h3><p><img src="/assets/image-20240805144745-puok93k.png" alt="image">​</p>
<p>欸这个就非常有意思了。 <strong>在模型部署的初始化阶段（推理正式开始前），vLLM会通过模拟实验的方式，来决定gpu&#x2F;cpu上到底有多少个KV cache物理块可以分配给后续的请求们做推理。vLLM管这个步骤叫determine_num_available_blocks</strong> 。我们来看看这个模拟实验是怎么做的：</p>
<p><strong>（1）杜撰假数据</strong></p>
<p><strong>（2）用假数据模拟一次前向推理</strong></p>
<p><strong>我们现在想知道在1次推理过程中，可以分配多少的显存给KV cache。我们可以使用如下公式计算：</strong><br><strong>分配给KV cache显存 &#x3D; gpu总显存 - 不使用KV cache做1次推理时的显存占用（包括模型本身和推理过程中的中间数据）</strong></p>
<p>对于“不使用 KV cache 做 1 次推理时的显存占用”，我们就可以用杜撰出来的假数据模拟一次前向推理来计算得出。在前向推理之后，我们把 gpu 上的缓存清一次，让它不要影响后续模型的正常推理。</p>
<p><strong>（3）计算可分配的KV cache物理块总数</strong></p>
<p>从（2）的模拟实验中，我们已经预估了一块卡上“分配给 KV Cache 的总显存”。现在，我们可以来计算总的物理块数量了。<br>我们易知：<strong>总物理块数量 &#x3D; 分配给KV Cache的显存大小&#x2F; 物理块大小，其中“大小”的单位是bytes。</strong></p>
<p>物理块尺寸（block_size），也即一个物理块有多少个槽位，是可以由用户自定义的，vLLM 推荐的默认值是 block_size &#x3D; 16。由大模型中 KV 值的定义，我们易知：<br><code>K_cache_block_size =&lt;span&gt; &lt;/span&gt;&lt;i&gt;block_size * num_heads * head_size * num_layers * dtype_size&lt;/i&gt;</code><br>其中 dtype_size 表示精度对应的大小，例如 fp16 就是 2，fp32 就是 4<br>同理可知：<code>V_cache_block_size = K_cache_block_size</code><br>则最终一个物理块的大小为：<br><code>cache_block_size =&lt;span&gt; &lt;/span&gt;&lt;i&gt;block_size * num_heads * head_size * num_layers * dtype_size * 2&lt;/i&gt;</code></p>
<p>知道了物理块的大小，我们就能求出物理块的总数了。</p>
<p>CPU 上物理块总数也是同理，但与 GPU 不同的是，它不需要做模拟实验。CPU 上可用的内存总数是用户通过参数传进来的（默认是 4G）。也就是我们认为只能在这 4G 的空间上做 swap。将上面公式中“分配给 KV Cache 的显存大小”替换成 4G，就能得到 CPU 上物理块的数量。</p>
<p><strong>（4）将预分配的KV Cache加载到gpu上</strong></p>
<p><img src="/assets/image-20240805144757-4vhudkx.png" alt="image">​</p>
<p> <strong>当我们确定好KV Cache block的大小后，我们就可以创建empty tensor，将其先放置到gpu上，实现显存的预分配。以后这块显存就是专门用来做KV Cache的了。</strong> 也正是因为这种预分配，你可能会发现在 vLLM 初始化后，显存的占用比你预想地要多（高过模型大小），这就是预分配起的作用。相关代码如下（cache_engine.py）:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def _allocate_kv_cache(</span><br><span class="line">    self,</span><br><span class="line">    num_blocks: int,</span><br><span class="line">    device: str,</span><br><span class="line">) -&gt; List[torch.Tensor]:</span><br><span class="line">    &quot;&quot;&quot;Allocates KV cache on the specified device.&quot;&quot;&quot;</span><br><span class="line">    kv_cache_shape = self.attn_backend.get_kv_cache_shape(</span><br><span class="line">        num_blocks, self.block_size, self.num_heads, self.head_size)</span><br><span class="line">    pin_memory = is_pin_memory_available() if device == &quot;cpu&quot; else False</span><br><span class="line">    kv_cache: List[torch.Tensor] = []</span><br><span class="line">    # =======================================================================</span><br><span class="line">    # kv_cache_shape: (2, num_blocks, block_size * num_kv_heads * head_size)</span><br><span class="line">    # =======================================================================</span><br><span class="line">    for _ in range(self.num_layers):</span><br><span class="line">        kv_cache.append(</span><br><span class="line">            torch.empty(kv_cache_shape,</span><br><span class="line">                        dtype=self.dtype,</span><br><span class="line">                        pin_memory=pin_memory,</span><br><span class="line">                        device=device))</span><br><span class="line">    return kv_cache</span><br></pre></td></tr></table></figure>

<p><strong>整个预分配的过程，其实也是在提醒我们：当你发现vLLM推理吞吐量可能不及预期，或者出现难以解释的bug时，可以先查查输出日志中pending(waiting)&#x2F;running&#x2F;swapped的序列数量，以及此时KV Cache部分的显存利用程度，尝试分析下这些默认的预分配设置是不是很好契合你的推理场景，如果不行，可以先尝试调整这些参数进行解决。</strong></p>
<h2 id="四、Scheduler-调度"><a href="#四、Scheduler-调度" class="headerlink" title="四、Scheduler 调度"></a>四、Scheduler 调度</h2><p>好，目前为止，vLLM 所有初始化的工作都完成了，我们现在可以来处理一条请求了。这就是我们调度器发挥作用的时候了，整个调度过程如下：</p>
<p><img src="/assets/image-20240805144807-b2gy9ma.png" alt="image">​</p>
<p><img src="/assets/image-20240805144815-ie8rbrg.png" alt="image">​</p>
<p>具体的内容我们在前文说了很多了。 <strong>这里只提一点：你会发现这出现了叫swapped的队列，这是前文没有提过的</strong> 。</p>
<p>如果你读过 vLLM 的原理篇， <strong>你可能记得vLLM的调度策略中有一项叫做：后来先抢占（ Preemption ）</strong> 。它是指在准备执行当前这 1 个推理阶段时，如果 gpu 上没有足够的资源对 running 队列中的全部数据完成下 1 次推理，我们就取出 running 队列中最后来的数据，将它的 KV Cache swapped 到 CPU 上，同时将这个数据从 running 移到 swapped 中。<strong>我们重复执行这个步骤，直到当前gpu上有足够的KV Cache空间留给剩在running中的全部数据为止。</strong></p>
<p>而存放在 Swapped 队列中的数据，也会在后续 gpu 上有足够空间时，被重新加入 running 计算。</p>
<p>‍</p>
<h3 id="一、SequenceGroup"><a href="#一、SequenceGroup" class="headerlink" title="一、SequenceGroup"></a>一、SequenceGroup</h3><h3 id="3-1-原生输入"><a href="#3-1-原生输入" class="headerlink" title="3.1 原生输入"></a>3.1 原生输入</h3><p>在一般的推理场景中， <strong>我们通常给模型传1个prompt及相关的采样参数</strong> ，让模型来做推理。此时你的输入可能长下面这样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(&quot;To be or not to be,&quot;,</span><br><span class="line">SamplingParams(temperature=0.8, top_k=5, presence_penalty=0.2)),</span><br></pre></td></tr></table></figure>

<p> <strong>但在其余的场景中，模型decoding的策略可能更加复杂</strong> ，例如：</p>
<ul>
<li><strong>Parallel Sampling</strong> ：你传给模型 1 个 prompt，希望模型基于这个这个 prompt，给出 n 种不同的 output</li>
<li><strong>Beam Search</strong> ：你传给模型 1 个 prompt，在采用 Beam Search 时，每个推理阶段你都会产出 top k 个 output，其中 k 被称为 Beam width（束宽）。</li>
</ul>
<p>这些情况下，你传给模型的输入可能长下面这样：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Parallel Sampling</span><br><span class="line">(&quot;What is the meaning of life?&quot;,</span><br><span class="line">SamplingParams(n=2, temperature=0.8, top_p=0.95, frequency_penalty=0.1))</span><br><span class="line"></span><br><span class="line"># Beam Search (best_of = 束宽)</span><br><span class="line">(&quot;It is only with the heart that one can see rightly&quot;,</span><br><span class="line">SamplingParams(n=3, best_of=3, use_beam_search=True, temperature=0.0)),</span><br></pre></td></tr></table></figure>

<p>【备注：SamplingParams 遵从 OpenAI API 范式，对其中各种参数的解释可参见 <a href="https://link.zhihu.com/?target=https://platform.openai.com/docs/api-reference/completions">OpenAI官方文档</a>】</p>
<p><strong>总结来说，可能出现&quot;1个prompt -&gt; 多个outputs&quot;的情况。那是否能设计一种办法，对1个prompt下所有的outputs进行集中管理，来方便vLLM更好做推理呢？</strong></p>
<h3 id="3-2-SequenceGroup-的作用"><a href="#3-2-SequenceGroup-的作用" class="headerlink" title="3.2 SequenceGroup 的作用"></a>3.2 SequenceGroup 的作用</h3><ul>
<li><p><strong>&quot;1个prompt -&gt; 多个outputs&quot;这样的结构组成一个SequenceGroup实例。</strong></p>
</li>
<li><p><strong>其中每组&quot;prompt -&gt; output&quot;组成一个序列（seq，属于Sequence实例），每个seq下有若干状态(status)属性，包括：</strong></p>
<ul>
<li><p><strong>WAITING：</strong> 正在 waiting 队列中。waiting 队列中的序列都没有做过 prefill。</p>
</li>
<li><p><strong>RUNNING：</strong> 正在 running 队列中，即已经开始做推理。</p>
</li>
<li><p><strong>SWAPPED：</strong> 正在 swapped 队列中，表示此时 gpu 资源不足，相关的 seq_group 被抢占，导致其暂停推理，相关的 KV block 被置换到 cpu 上（swap out），等待 gpu 资源充足时再置换回来重新计算（swap in）。</p>
</li>
<li><p><strong>若干和Finish相关的状态</strong> ，表示该 seq 推理已经结束，具体包括：</p>
<ul>
<li><strong>FINISHED_STOPPED：</strong> 正常执行完毕，例如碰到 <code>&lt;eos&gt;</code> 符号，该 seq 的推理正常结束了</li>
<li><strong>FINISHED_LENGTH_CAPPED</strong> ：因为 seq 的长度达到最大长度限制，而结束推理</li>
<li><strong>FINISHED_ABORTED</strong> ：因不正常状态，而被终止的推理。例如客户端断开连接，则服务器会终止相关 seq 的推理</li>
<li><strong>FINISHED_IGNORED</strong> ：因 prompt 过长而被终止执行的推理。本质上也是受到长度限制</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>在vLLM中有一个重要假设：一个seq_group中的所有seq共享1个prompt。</strong></p>
</li>
</ul>
<p>我们来通过一个具体的例子，更好感受一下 SequenceGroup 的作用：</p>
<p><img src="/assets/image-20240806100519-rzb5fc6.png" alt="image">​</p>
<ul>
<li><p><strong>在推理开始之前</strong> ，这个 seq_group 下只有 1 条 seq，它就是 prompt，状态为 waiting。</p>
</li>
<li><p><strong>在第1个推理阶段</strong> ，调度器选中了这个 seq_group，由于它的采样参数中 <code>n = 4</code>，所以在做完 prefill 之后，它会生成 4 个 seq，它们的状态都是 running。</p>
</li>
<li><p><strong>在若干个推理阶段后，gpu上的资源不够了，这个seq_group不幸被调度器抢占（preemption）</strong> ，它相关的 KV block 也被 swap out 到 cpu 上。此时所有 seq 的状态变为 swapped。这里要注意，<strong>当一个seq_group被抢占时，对它的处理有两种方式：</strong></p>
<ul>
<li><strong>Swap：如果该seq_group下的seq数量 &gt; 1，此时会采取swap策略</strong> ，即把 seq_group 下【所有】seq 的 KV block 从 gpu 上卸载到 cpu 上。（seq 数量比较多，直接把算出的 KV block 抛弃，比较可惜）</li>
<li><strong>Recomputation：如果该seq_group下的seq数量 &#x3D; 1，此时会采取recomputation策略</strong> ，即把该 seq_group 相关的物理块都释放掉，然后将它重新放回 waiting 队列中。等下次它被选中推理时，就是从 prefill 阶段开始重新推理了，因此被称为“重计算”。（seq 数量少，重新计算 KV block 的成本不高）</li>
</ul>
</li>
</ul>
<p><strong>【注意，并不是每个seq_group都会经历抢占，具体要看调度器策略和gpu资源使用情况】</strong></p>
<ul>
<li><strong>又过了若干个推理阶段，gpu上的资源又充足了，此时执行swap in操作</strong> ，将卸载到 cpu 上的 KV block 重新读到 gpu 上，继续对该 seq_group 做推理，此时 seq 的状态又变为 running。</li>
<li><strong>又过了若干个推理阶段，该seq_group中有1个seq已经推理完成了，它的状态就被标记为finish</strong> ，此后这条已经完成的 seq 将不参与调度。</li>
<li><strong>又过了若干个推理阶段，这个seq_group下所有的seq都已经完成推理了</strong> ，这样就可以把它作为最终 output 返回了。</li>
</ul>
<p>相信通过这个例子，我们已经能更好理解为什么 vLLM 要把 1 个 prompt 包装成 SequenceGroup 实例了。接下来我们就来看 SequenceGroup 实例的具体结构。</p>
<h3 id="3-3-SequenceGroup-的结构"><a href="#3-3-SequenceGroup-的结构" class="headerlink" title="3.3 SequenceGroup 的结构"></a>3.3 SequenceGroup 的结构</h3><p>SequenceGroup 相关的脚本在 <code>vllm/sequence.py</code> 中，下图给出了 SequenceGroup 的结构图解（ <strong>仅列出重要的属性和方法</strong> ）：</p>
<p><img src="/assets/image-20240806100535-su3mv44.png" alt="image">​</p>
<p><strong>（1）结构总述</strong></p>
<p><strong>SequenceGroup:</strong></p>
<ul>
<li><strong>self.seqs_dict</strong> ：{seq_id: seq}，其中每个 seq 是一个 Sequence 对象。正如我们前文介绍的那样，一个 seq_group 下包含若干 seqs</li>
<li><strong>self.sampling_params</strong> ：采样参数</li>
<li><strong>self.metrics</strong> ： <strong>记录该seq_group相关的指标，例如该seq_group是什么时候被加入LLMEngine的（arrival_time）</strong> ，该 seq_group 第一次被调度器选中调度是什么时候等等。调度器在选择时，会参考 seq_groups 们的这些指标来做决策。</li>
<li><strong>get_max_num_running_steps</strong> ： <strong>该seq_group在剩余生命周期内并行running的最大seq数量</strong> 。 <strong>“剩余生命周期”指从此刻一直到seq_group中所有的seq都做完推理</strong> 。举个例子来说，我们看 2.2 节配图中倒数第 3 个时刻，此时这个 seq_group 内所有的 seq 都还没结束推理，所以若调用这个方法，则返回值为 4；再看倒数第 2 个时刻，此时有 1 个 seq 已经完成了推理，所以若调用这个方法，则返回值为 3。在后续调度策略代码中，我们将经常看到这个方法被调用，目的是用于估计若当前对一个 seq_group 做推理，它将消耗多少 gpu 资源。</li>
</ul>
<p>我们来详细看下 <strong>get_max_num_running_steps</strong> 代码实现（一切尽在注释中）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">def get_max_num_running_seqs(self) -&gt; int:</span><br><span class="line">    &quot;&quot;&quot;The maximum number of sequences running in parallel in the remaining</span><br><span class="line">    lifetime of the request.</span><br><span class="line">    返回请求在其剩余生命周期中并行运行的最大序列数。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # ============================================================================</span><br><span class="line">    # 若采用beam search，每1个推理阶段都是best_of（束宽）个seq在running</span><br><span class="line">    # ============================================================================</span><br><span class="line">    if self.sampling_params.use_beam_search:</span><br><span class="line">        return self.sampling_params.best_of</span><br><span class="line">    # ============================================================================</span><br><span class="line">    # 如果不采用beam search</span><br><span class="line">    # ============================================================================</span><br><span class="line">    else:</span><br><span class="line">        # =========================================================================</span><br><span class="line">        # 此时best_of默认和n一致，即表示我们希望1个prompt产出n个outputs。因此理论上，这个</span><br><span class="line">        # seq_group下会维护best_of个seq（这就是self.num_seqs()的返回值）。</span><br><span class="line">        # 如果出现best_of &gt; self.num_seqs()的情况，说明该seq_group刚从waiting变成running</span><br><span class="line">        # 准备做推理（参考2.2节配图中左侧第1个时刻），此时对于这个seq_group来说，</span><br><span class="line">        # 其剩余生命周期并行运行的最大seq数量为best_of</span><br><span class="line">        # =========================================================================</span><br><span class="line">        if self.sampling_params.best_of &gt; self.num_seqs():</span><br><span class="line">            # At prompt stage, the sequence group is not yet filled up</span><br><span class="line">            # and only have one sequence running. However, in the</span><br><span class="line">            # generation stage, we will have `best_of` sequences running.</span><br><span class="line">            return self.sampling_params.best_of</span><br><span class="line">      </span><br><span class="line">        # =========================================================================</span><br><span class="line">        # 其余时刻（例如2.2节配图中非左侧第1个时刻的所有时刻）下，我们就返回这个seq_group中</span><br><span class="line">        # 未完成推理的seq数量。根据2.2节介绍，我们知道一个seq的完成状态有四种：</span><br><span class="line">        #   SequenceStatus.FINISHED_STOPPED,</span><br><span class="line">        #   SequenceStatus.FINISHED_LENGTH_CAPPED,</span><br><span class="line">        #   SequenceStatus.FINISHED_ABORTED,</span><br><span class="line">        #   SequenceStatus.FINISHED_IGNORED</span><br><span class="line">        # =========================================================================</span><br><span class="line">        return self.num_unfinished_seqs()</span><br></pre></td></tr></table></figure>

<p><strong>Sequence:</strong></p>
<p>对于一个 seq，我们重点来看它的属性 <code>self.logical_token_blocks</code>（逻辑块）和方法 <code>_append_tokens_to_blocks</code>（生成逻辑块的方法）。 <strong>在vLLM中，每个seq都单独维护一份属于自己的逻辑块，不同的逻辑块可以指向同一个物理块</strong> （此刻你一定很关心逻辑块和物理块是如何做映射的，我们会循序渐进地讲解这点， <strong>现在你可以先忽略映射方法，把目光聚焦于“一个seq的逻辑块长什么样，怎么初始化它的逻辑块”</strong> ）</p>
<p><strong>（2）1个逻辑块的结构</strong></p>
<p> <strong>我们先来回答“1个逻辑块长什么样”这个问题</strong> ，逻辑块定义的代码比较简单，所以我们直接看代码（一切尽在注释中），代码路径 <code>vllm/block.py</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">class LogicalTokenBlock:</span><br><span class="line">    &quot;&quot;&quot;A block that stores a contiguous chunk of tokens from left to right.</span><br><span class="line"></span><br><span class="line">    Logical blocks are used to represent the states of the corresponding</span><br><span class="line">    physical blocks in the KV cache.</span><br><span class="line">  </span><br><span class="line">    KV cache的逻辑块</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        block_number: int, # 逻辑块的序号</span><br><span class="line">        block_size: int, # 每个逻辑块中有多少个槽位（默认为16）</span><br><span class="line">    ) -&gt; None:</span><br><span class="line">        self.block_number = block_number</span><br><span class="line">        self.block_size = block_size</span><br><span class="line"></span><br><span class="line">        # 逻辑块刚初始化时，将其中的每个token_id都初始化为_BLANK_TOKEN_ID（-1）</span><br><span class="line">        self.token_ids = [_BLANK_TOKEN_ID] * block_size </span><br><span class="line">        # 当前逻辑块中已经装下的token的数量</span><br><span class="line">        self.num_tokens = 0</span><br><span class="line"></span><br><span class="line">    def is_empty(self) -&gt; bool:</span><br><span class="line">        &quot;&quot;&quot;判断当前逻辑块是为空&quot;&quot;&quot;</span><br><span class="line">        return self.num_tokens == 0</span><br><span class="line"></span><br><span class="line">    def get_num_empty_slots(self) -&gt; int:</span><br><span class="line">        &quot;&quot;&quot;当前逻辑块的空余槽位&quot;&quot;&quot;</span><br><span class="line">        return self.block_size - self.num_tokens</span><br><span class="line"></span><br><span class="line">    def is_full(self) -&gt; bool:</span><br><span class="line">        &quot;&quot;&quot;判断当前逻辑块是否已经被装满&quot;&quot;&quot;</span><br><span class="line">        return self.num_tokens == self.block_size</span><br><span class="line"></span><br><span class="line">    def append_tokens(self, token_ids: List[int]) -&gt; None:</span><br><span class="line">        &quot;&quot;&quot;将给定的一些token_ids装入当前逻辑块中&quot;&quot;&quot;</span><br><span class="line">        # 给定的token_ids的长度必须 &lt;= 当前逻辑块剩余的槽位</span><br><span class="line">        assert len(token_ids) &lt;= self.get_num_empty_slots()</span><br><span class="line">        # 当前逻辑块第一个空槽的序号</span><br><span class="line">        curr_idx = self.num_tokens</span><br><span class="line">        # 将这些tokens装进去</span><br><span class="line">        self.token_ids[curr_idx:curr_idx + len(token_ids)] = token_ids</span><br><span class="line">        # 更新当前逻辑块中tokens的数量</span><br><span class="line">        self.num_tokens += len(token_ids)</span><br><span class="line"></span><br><span class="line">    def get_token_ids(self) -&gt; List[int]:</span><br><span class="line">        &quot;&quot;&quot;获取当前逻辑块中所有被装满的位置的token_ids&quot;&quot;&quot;</span><br><span class="line">        return self.token_ids[:self.num_tokens]</span><br><span class="line"></span><br><span class="line">    def get_last_token_id(self) -&gt; int:</span><br><span class="line">        &quot;&quot;&quot;获取当前逻辑块所所有被装满的位置的最后一个token_id&quot;&quot;&quot;</span><br><span class="line">        assert self.num_tokens &gt; 0</span><br><span class="line">        return self.token_ids[self.num_tokens - 1]</span><br></pre></td></tr></table></figure>

<p><strong>（3）再回到Sequence上来</strong></p>
<p>知道了每个逻辑块的结构， <strong>我们现在来回答“怎么给一个seq分配逻辑块”这个问题</strong> ，也就是回到 2.3（1）中 Sequence 的**_append_tokens_to_blocks** 方法上来： <strong>当一个seq只有prompt时，这个方法负责给prompt分配逻辑块；当这个seq开始产出output时，这个方法负责给每一个新生成的token分配逻辑块</strong> ，整个过程如下图（图片来自 vLLM 论文，大家忽略图中 block_table 的部分）：</p>
<p><img src="/assets/image-20240806100557-97wlhl7.png" alt="image">​</p>
<p>代码如下（一切尽在注释中，<code>/vllm/sequence.py</code>）:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def _append_tokens_to_blocks(self, token_ids: List[int]) -&gt; None:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    将token_ids动态填入逻辑块列表中</span><br><span class="line">    Args:</span><br><span class="line">        token_ids: prompt部分的token_ids</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    cursor = 0</span><br><span class="line">    # 遍历prompt token_ids中的每一个token_id</span><br><span class="line">    while cursor &lt; len(token_ids):</span><br><span class="line">        # 如果当前逻辑块列表（logical_token_blocks）为空</span><br><span class="line">        if not self.logical_token_blocks:</span><br><span class="line">            # 则先append一个逻辑块，该逻辑块index为0，大小为16，其中的每一个token_id为-1</span><br><span class="line">            self._append_logical_block()</span><br><span class="line"></span><br><span class="line">        # 取出逻辑块列表中的最后一个逻辑块</span><br><span class="line">        last_block = self.logical_token_blocks[-1]</span><br><span class="line">        # 如果这最后一个逻辑块中已经没有槽位</span><br><span class="line">        if last_block.is_full():</span><br><span class="line">            # 那么再append一个逻辑块，其大小为16，其中每一个token_id为-1</span><br><span class="line">            self._append_logical_block()</span><br><span class="line">            # 把这个新append的逻辑块取出来</span><br><span class="line">            last_block = self.logical_token_blocks[-1]</span><br><span class="line">      </span><br><span class="line">        # 检查当前取出的逻辑块中空槽位的数量</span><br><span class="line">        num_empty_slots = last_block.get_num_empty_slots()</span><br><span class="line">        # 用当前的token_ids填充空槽位，直到无法填满为止</span><br><span class="line">        last_block.append_tokens(token_ids[cursor:cursor +</span><br><span class="line">                                           num_empty_slots])</span><br><span class="line">        cursor += num_empty_slots</span><br></pre></td></tr></table></figure>

<p><strong>好，到目前为止，我们就把vLLM对输入数据做预处理的部分介绍完了，简单总结下：</strong></p>
<ul>
<li><strong>在vLLM内部计算逻辑中，1个prompt是1个request</strong></li>
<li><strong>每个prompt将被包装成一个SequenceGroup实例提供给调度器做调度</strong></li>
<li><strong>1个SequenceGroup实例下维护着若干个Sequence实例，对应着“1个prompt -&gt; 多个outputs&quot;这种更一般性的解码场景。</strong></li>
<li><strong>1个Sequence实例下维护着属于自己的逻辑块列表，数据类型为List[LogicalTokenBlock]</strong></li>
</ul>
<h2 id="三、add-request-：将-seq-group-添加进调度器-waiting-队列"><a href="#三、add-request-：将-seq-group-添加进调度器-waiting-队列" class="headerlink" title="三、add_request()：将 seq_group 添加进调度器 waiting 队列"></a>三、add_request()：将 seq_group 添加进调度器 waiting 队列</h2><p>写了这么多，你是不是已经忘记上面都说了些什么了，不要紧，我们快速回顾下：</p>
<ul>
<li><strong>首先，我们明确了vLLM最重要的推理内核引擎是LLMEngine</strong></li>
<li><strong>LLMEngine下有两个最重要的方法：add_request()和step()</strong></li>
<li><strong>add_request()负责将每个prompt都包装成一个SequenceGroup对象，送入调度器的waiting队列中等待调度</strong></li>
<li><strong>step()负责执行1次推理过程，在这个过程中，调度器首先决定哪些seq_group可以被送去推理，然后model_executor负责实际执行推理。</strong></li>
</ul>
<p>现在，在知道 SequenceGroup 相关定义的基础上，我们可以来看 <code>add_request()</code> 了，我们直接来看代码（一切尽在注释中，为了方便阅读，代码有所省略）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"># vllm/engine/llm_engine.py</span><br><span class="line">    def add_request(</span><br><span class="line">        self,</span><br><span class="line">        request_id: str, # 每个请求的唯一id</span><br><span class="line">        prompt: Optional[str], # prompt（文字版）</span><br><span class="line">        sampling_params: SamplingParams, # 用于采样的参数（温度、topk等）</span><br><span class="line">        prompt_token_ids: Optional[List[int]] = None, # prompt（input_ids版）</span><br><span class="line">        arrival_time: Optional[float] = None, # 请求到达的时间。如果是None，则用当前系统时间</span><br><span class="line">        lora_request: Optional[LoRARequest] = None,  # 如果是用lora模型做推理，相关的lora请求</span><br><span class="line">        multi_modal_data: Optional[MultiModalData] = None, # 每个请求的多模态数据</span><br><span class="line">    ) -&gt; None:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        将request添加给LLMEngine</span><br><span class="line"></span><br><span class="line">        Args:</span><br><span class="line">            request_id: 在vLLM内部，1条prompt算1个请求，会附给1个请求id</span><br><span class="line">            prompt: prompt（文字版）</span><br><span class="line">            sampling_params: 采样参数（温度、topk等）</span><br><span class="line">            prompt_token_ids: prompt（token_id版），没有提供的话vLLM会调用tokenizer来做</span><br><span class="line">            arrival_time: 请求到达的时间。如果是None，则用当前系统时间</span><br><span class="line">            multi_modal_data: 多模态数据（暂时忽略不看）</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        ...</span><br><span class="line">      </span><br><span class="line">        # ============================================================================</span><br><span class="line">        # 设置该请求的到达时间</span><br><span class="line">        # ============================================================================</span><br><span class="line">        if arrival_time is None:</span><br><span class="line">            arrival_time = time.time()</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        # 每个KV cache block的大小（默认为16）</span><br><span class="line">        block_size = self.cache_config.block_size</span><br><span class="line">        # 当前seq的id（见后文讲解）</span><br><span class="line">        seq_id = next(self.seq_counter)</span><br><span class="line">        # 获取用于表示&lt;eos&gt;的token_id</span><br><span class="line">        eos_token_id = self.tokenizer.get_lora_tokenizer(</span><br><span class="line">            lora_request).eos_token_id</span><br><span class="line">      </span><br><span class="line">        # ============================================================================</span><br><span class="line">        # 为当前序列创建Sequence对象，在Sequence对象中也包括对当前序列逻辑块们的管理</span><br><span class="line">        # ============================================================================</span><br><span class="line">        seq = Sequence(seq_id, prompt, prompt_token_ids, block_size,</span><br><span class="line">                       eos_token_id, lora_request)</span><br><span class="line">        ...</span><br><span class="line">        # ============================================================================</span><br><span class="line">        # 每个prompt被包装成一个SequenceGroup实例</span><br><span class="line">        # ============================================================================</span><br><span class="line">        seq_group = SequenceGroup(request_id, [seq], sampling_params,</span><br><span class="line">                                  arrival_time, lora_request, multi_modal_data)</span><br><span class="line"></span><br><span class="line">        # ============================================================================</span><br><span class="line">        # 将seq_group中所有序列添加进scheduler的self.waiting队列中</span><br><span class="line">        # self.waiting是一个双端队列实例，我们可以在队列的两端进行插入/删除操作</span><br><span class="line">        # ============================================================================</span><br><span class="line">        self.scheduler.add_seq_group(seq_group)</span><br></pre></td></tr></table></figure>

<h2 id="四：step-：调度器策略"><a href="#四：step-：调度器策略" class="headerlink" title="四：step()：调度器策略"></a>四：step()：调度器策略</h2><p>现在所有的 seq_group 都已经被送入调度器（Scheduler）的 waiting 队列中了， <strong>接下来我们就来看，在1个推理阶段中，调度器是通过什么策略来决定要送哪些seq_group去做推理的</strong> ，这也是 vLLM 难啃的硬骨头之一。</p>
<p> <strong>调度器相关的代码都在vllm&#x2F;core&#x2F;scheduler.py中，由于代码逻辑嵌套比较复杂，所以我们依然先通过图解的方式把整个调度流程介绍一遍，然后再看关键的源码细节</strong> 。</p>
<h3 id="4-1-调度器结构"><a href="#4-1-调度器结构" class="headerlink" title="4.1 调度器结构"></a>4.1 调度器结构</h3><p><img src="/assets/image-20240806100615-zq9upxf.png" alt="image">​</p>
<p>vLLM 调度器维护的重要属性如上图所示：</p>
<ul>
<li><p><strong>self.waiting, self.running, self.swapped</strong> ：这三个都是 python 的 deque()实例（双端队列，允许你从队列两侧添加或删除元素）。</p>
<ul>
<li><strong>waiting队列用于存放所有还未开始做推理的seq_group</strong> ，“未开始”指连 prefill 阶段都没有经历过。所以 waiting 队列中的 seq_group 只有一个 seq，即是原始的 prompt。</li>
<li><strong>running队列用于存放当前正在做推理的seq_group。更准确地说，它存放的是上1个推理阶段被送去做推理的seq_group们</strong> ，在开始新一轮推理阶段时，调度器会根据本轮的筛选结果，更新 running 队列，即决定本轮要送哪些 seq_group 去做推理。</li>
<li><strong>swapped队列用于存放被抢占的seq_group</strong> 。在 2.2 节中我们有提过，若一个 seq_group 被抢占，调度器会对它执行 swap 或 recomputation 操作，分别对应着将它送去 swapped 队列或 waiting 队列，在后文我们会详细分析抢占处理的代码</li>
</ul>
</li>
<li><p><strong>self.policy：是vLLM自定义的一个Policy实例，</strong> 目标是根据调度器总策略（ <strong>FCFS</strong> ，First Come First Serve，先来先服务）原则， <strong>对各个队列里的seq_group按照其arrival time进行排序</strong> 。相关代码比较好读，所以这里我们只概述它的作用，后续不再介绍它的代码实现。</p>
</li>
<li><p><strong>self.prev_time</strong> ： <strong>上一次调度发起的时间点，初始化为0。</strong> 我们知道每执行 1 次推理阶段前，调度器都要做一次调度，这个变量存放的就是上次调度发起的时间点。</p>
</li>
<li><p><strong>self.prev_prompt</strong> ：取值为 True&#x2F;False，初始化为 False。<strong>若上一次调度时，调度器有从waiting队列中取出seq_group做推理，即为True，否则为False。</strong></p>
</li>
<li><p><strong>self.last_prompt_latency</strong> ： <strong>记录“当前调度时刻（now） - 最后一次有从waiting队列中取数做推理的那个调度时刻”的差值</strong> （并不是每一次调度时，调度器一定都会从 waiting 队列中取 seq_group，它可能依旧继续对 running 队列中的数据做推理），初始化为 0。</p>
</li>
</ul>
<p>目前你可能很难明白这三个属性的作用，不要着急，在后文讲解具体调度流程时，我们会再来看它们。这里只需记住它们的定义即可。</p>
<ul>
<li><p><strong>BlockManager</strong> ： <strong>物理块管理器</strong> 。这也是 vLLM 自定义的一个 class。截止本文写作时，vLLM 提供了 <code>BlockSpaceManagerV1</code> 和 <code>BlockSpaceManagerV2</code> 两个版本的块管理器。V1 是 vLLM 默认的版本，V2 是改进版本（但还没开发完，例如不支持 prefix caching 等功能）。所以本文依然基于 <code>BlockSpaceManagerV1</code> 进行讲解。物理块管理器这个 class 下又维护着两个重要属性：</p>
<ul>
<li><strong>BlockAllocator：物理块分配者，负责实际为seq做物理块的分配、释放、拷贝等操作。</strong> 这也是我们后文要解读的对象。其下又分成 <code>self.gpu_allocator</code> 和 <code>self.cpu_allocator</code> 两种类型，分别管理 gpu 和 cpu 上的物理块。</li>
<li><strong>self.block_tables：负责维护每个seq下的物理块列表，本质上它是一个字典，形式如{seq_id: List[PhysicalTokenBlock]}。</strong> 注意，这里维护者【所有】seq_group 下 seq 的物理块，而不是单独某一个 seq 的。因为整个调度器都是全局的，其下的 BlockManager 自然也是全局的。</li>
</ul>
</li>
</ul>
<p>读到这里， <strong>你还记得2.3节中我们曾介绍过，每个Sequence实例中维护着属于这个seq的逻辑块吗？而我们从self.block_tables中，又能根据seq_id找到这个seq对应的物理块。这就实现了“逻辑块 -&gt; 物理块”的映射</strong> 。在刚开始读代码的时候，很多朋友从直觉上都会觉得 <strong>BlockManager</strong> 就是用来存储逻辑块和物理块映射的，其实它 <strong>只负责管理和分配物理块，映射关系潜藏在seq中</strong> 。理解这点对理解代码非常重要。</p>
<p>现在，我们就把调度器（Scheduler）的结构理清了。我知道你肯定还有很多疑惑。所以我们马上来看调度策略的具体流程：<strong>“对于装在waiting、running、swapped队列中的那些seq_group，是根据什么规则决定本次推理阶段该送谁去推理呢？”</strong></p>
<h3 id="4-2-整体调度流程"><a href="#4-2-整体调度流程" class="headerlink" title="4.2 整体调度流程"></a>4.2 整体调度流程</h3><p><img src="/assets/image-20240806100627-vlg00vs.png" alt="image">​</p>
<p>上图刻画了某次调度步骤中三个队列的情况，再复习一下：</p>
<ul>
<li><strong>waiting队列</strong>中的数据都没有做过 prefill，每个 seq_group 下只有 1 个 seq（prompt）</li>
<li><strong>running队列</strong>中存放着上一个推理阶段被送去做推理的所有 seq_group</li>
<li><strong>swapped队列</strong>中存放着之前调度阶段中被抢占的 seq_group</li>
</ul>
<p> <strong>running队列中的seq_group不一定能继续在本次调度中被选中做推理</strong> ，这是因为 gpu 上 KV cache 的使用情况一直在变动，以及 waiting 队列中持续有新的请求进来的原因。所以调度策略的职责就是要根据这些变动，对送入模型做推理的数据做动态规划。</p>
<p><img src="/assets/image-20240806100641-76xmhlz.png" alt="image">​</p>
<p>根据源码，我将 vLLM 调度步骤整理成上述流程图。看着有点复杂是吧，不要担心，我们这就来拆解它。</p>
<p>总结来说：</p>
<ul>
<li><p><strong>如果当前swapped队列为空，那就去检查是否能从waiting队列中调度seq_group，直到不满足调度条件为止（gpu空间不足，或waiting队列已为空等）</strong> 。<strong>此时，1个推理阶段中，所有的seq_group都处在prefill阶段。</strong></p>
</li>
<li><p><strong>如果当前swapped队列非空，或者无法从waiting队列中调度任何seq_group时：</strong></p>
<ul>
<li>检查是否能从 running 队列中调度 seq_group，直到不满足调度条件为止。</li>
<li>若本次无新的被抢占的 seq_group，且 swapped 队列非空，就检查是否能从 swapped 队列中调度 seq_group，直到不满足调度条件为止。</li>
</ul>
</li>
</ul>
<p><strong>此时，1个推理阶段中，所有的seq_group要么全来自running队列，要么来自running + swapped队列，它们都处在decode阶段。</strong></p>
<p><strong>至此我们要记住vLLM调度中非常重要的一点：在1个推理阶段中，所有的seq_group要么全部处在prefill阶段。要么全部处在decode阶段。</strong></p>
<p>你可能想问：<strong>为什么要以swapped是否非空为判断入口呢？</strong><br>这是因为，如果当前调度步骤中 swapped 队列非空，说明在之前的调度步骤中这些可怜的 seq_group 因为资源不足被抢占，而停滞了推理。所以<strong>根据FCFS规则，当gpu上有充足资源时，我们应该先考虑它们，而不是考虑waiting队列中新来的那些seq_group。</strong><br>同理，在图中你会发现，当我们进入对 running 队列的调度时（图中红色分支），我们会根据“ <strong>本次调度是否有新的被抢占的seq_group</strong> ”，来决定要不要调度 swapped 队列中的数据。这个理由也很简单：在本次调度中，我就是因为考虑到 gpu 空间不足的风险，我才新抢占了一批序列。既然存在这个风险，我就最好不要再去已有的 swapped 队列中继续调度 seq_group 了。</p>
<p><strong>到这里，我们已经把整个调度流程的关键点给说完了。接下来，我们会配合源码，对上图中的细节进行介绍。</strong></p>
<h3 id="4-3-passed-delay：判断调度-waiting-队列的时间点"><a href="#4-3-passed-delay：判断调度-waiting-队列的时间点" class="headerlink" title="4.3 _passed_delay：判断调度 waiting 队列的时间点"></a>4.3 _passed_delay：判断调度 waiting 队列的时间点</h3><p><img src="/assets/image-20240806100708-0as566l.png" alt="image">​</p>
<p>在 4.2 的流程图中，我们会看到 <strong>进入waiting循环的判断条件之一是：waiting队列是否达到调度间隔阈值</strong> 。这是个什么东西？又为什么要设置这样一个阈值呢？</p>
<p>我们知道模型在做推理时，waiting 队列中是源源不断有 seq_group 进来的，一旦 vLLM 选择调度 waiting 队列，它就会停下对 running&#x2F;swapped 中 seq_group 的 decode 处理，转而去做 waiting 中 seq_group 的 prefill，也即 <strong>vLLM必须在新来的seq_group和已经在做推理的seq_group间取得一种均衡：既不能完全不管新来的请求，也不能耽误正在做推理的请求。所以“waiting队列调度间隔阈值”就是来控制这种均衡的：</strong></p>
<ul>
<li><strong>调度间隔设置得太小</strong> ，每次调度都只关心 waiting 中的新请求，这样发送旧请求的用户就迟迟得不到反馈结果。且此时 waiting 队列中积累的新请求数量可能比较少，不利于做 batching，浪费了并发处理的能力。</li>
<li><strong>调度间隔设置得太大</strong> ，waiting 中的请求持续挤压，同样对 vLLM 推理的整体吞吐有影响。</li>
</ul>
<p>那这个阈值在代码中是怎么控制的呢？还记得 4.1 中我们画 Scheduler 的结构图时有三个乍一看比较难懂的属性吗（见下图），它们就是用来控制这个阈值的：</p>
<p><img src="/assets/image-20240806100721-l8vibv3.png" alt="image">​</p>
<p><code>vllm/core/scheduler.py</code> 脚本的 <code>_passed_delay()</code> 函数写了阈值判断的相关逻辑，我们直接看代码（一切尽在注释中）:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def _passed_delay(self, now: float) -&gt; bool:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    判断当下是否可以从waiting队列中调度新请求</span><br><span class="line">    这个函数确保了在调度过程中不会频繁地处理新来的seq_group</span><br><span class="line">  </span><br><span class="line">    Args:</span><br><span class="line">        now: 当前调度时间点</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # =============================================================================</span><br><span class="line">    # self.prev_prompt: True/False，记录上一次调度步骤中，是否选择了从waiting队列中做调度</span><br><span class="line">    # self.prev_time：上次调度步骤时间点（不管是从哪个队列中调度，每次调度都会记录下时间点）</span><br><span class="line">    # 若上个调度步骤中，我们选择从waiting队列中做调度，则计算两个调度时刻的间隔</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    if self.prev_prompt:</span><br><span class="line">        self.last_prompt_latency = now - self.prev_time</span><br><span class="line">  </span><br><span class="line">    # =============================================================================</span><br><span class="line">    # 用当前调度时间更新prev_time</span><br><span class="line">    # 由于目前还不知道本次是否会从waiting队列中调度，因此prev_prompt先设为False</span><br><span class="line">    # =============================================================================</span><br><span class="line">    self.prev_time, self.prev_prompt = now, False</span><br><span class="line">  </span><br><span class="line">    # =============================================================================</span><br><span class="line">    # Delay scheduling prompts to let waiting queue fill up</span><br><span class="line">    # delay_factor：用户配置的，用于调整调度间隔阈值的因子。大于0则意味着用户想开启阈值判断</span><br><span class="line">    # =============================================================================</span><br><span class="line">    if self.scheduler_config.delay_factor &gt; 0 and self.waiting:</span><br><span class="line">        # =========================================================================</span><br><span class="line">        # 计算在waiting队列中，最早到达的seq_group的到达时间</span><br><span class="line">        # =========================================================================</span><br><span class="line">        earliest_arrival_time = min(</span><br><span class="line">            [e.metrics.arrival_time for e in self.waiting])</span><br><span class="line">        # =========================================================================</span><br><span class="line">        # now - earliest_arrival_time：最早到达waiting队列的seq_group当前“实际”等待的时间</span><br><span class="line">        # delay_factor*last_prompt_latency：最早到达waiting队列的请求当前“应该”等待的时间</span><br><span class="line">        # 只要前者比后者大，或者此时running队列中根本没有请求在跑，就可以进行对waiting做调度</span><br><span class="line">        # =========================================================================</span><br><span class="line">        passed_delay = (</span><br><span class="line">            (now - earliest_arrival_time) &gt;</span><br><span class="line">            (self.scheduler_config.delay_factor * self.last_prompt_latency)</span><br><span class="line">            or not self.running)</span><br><span class="line">    # =============================================================================</span><br><span class="line">    # 如果你不想开启阈值判断，那就直接返回True</span><br><span class="line">    # =============================================================================</span><br><span class="line">    else:</span><br><span class="line">        passed_delay = True</span><br><span class="line">    return passed_delay</span><br></pre></td></tr></table></figure>

<h3 id="4-4-can-allocate：能否为-seq-group-分配物理块做-prefill"><a href="#4-4-can-allocate：能否为-seq-group-分配物理块做-prefill" class="headerlink" title="4.4 can_allocate：能否为 seq_group 分配物理块做 prefill"></a>4.4 can_allocate：能否为 seq_group 分配物理块做 prefill</h3><p><img src="/assets/image-20240806100731-g9wcgf1.png" alt="image">​</p>
<p>通过了调度时间阈值的判断条件，现在我们顺利从 waiting 中取出一个 seq_group，我们将对它进行 prefill 操作。 <strong>所以这里我们必须先判断：gpu上是否有充足的空间为该seq_group分配物理块做prefill</strong> ，根据 4.1 中绘制的调度器结构，这个操作当然是由我们的 self.block_manager 来做。</p>
<p>判断的入口代码为 <code>can_allocate = self.block_manager.can_allocate(seq_group)</code>，配合上面图例，我们直接来看 <code>can_allocate</code> 函数的代码，（一切尽在注释中）:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># vllm/core/block_manager_v1.py</span><br><span class="line">def can_allocate(self, seq_group: SequenceGroup) -&gt; AllocStatus:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    确实是否可以给这个seq_group分配物理块，返回结果有三种情况：</span><br><span class="line">    - AllocStatus.NEVER：不分配；</span><br><span class="line">    - AllocStatus.OK：可以分配；</span><br><span class="line">    - AllocStatus.LATER：延迟分配</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # FIXME(woosuk): Here we assume that all sequences in the group share</span><br><span class="line">    # the same prompt. This may not be true for preempted sequences.</span><br><span class="line">    # (这里我们假设一个seq_group下的所有序列的prompt都是相同的)</span><br><span class="line">  </span><br><span class="line">    # ===========================================================================</span><br><span class="line">    # 取出这个seq_group下处于waiting状态的序列</span><br><span class="line">    # ===========================================================================</span><br><span class="line">    seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]</span><br><span class="line">  </span><br><span class="line">    # ===========================================================================</span><br><span class="line">    # 取出这个seq所有的逻辑块</span><br><span class="line">    # ===========================================================================</span><br><span class="line">    num_required_blocks = len(seq.logical_token_blocks)</span><br><span class="line"></span><br><span class="line">    # ===========================================================================</span><br><span class="line">    # block上的滑动窗口（可暂时假设其值为None，先忽略不看</span><br><span class="line">    # ===========================================================================</span><br><span class="line">    if self.block_sliding_window is not None:</span><br><span class="line">        num_required_blocks = min(num_required_blocks,</span><br><span class="line">                                  self.block_sliding_window)</span><br><span class="line">    # ===========================================================================</span><br><span class="line">    # 计算当前所有可用的物理块数量，List[PhysicalTokenBlock]</span><br><span class="line">    # ===========================================================================</span><br><span class="line">    num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()</span><br><span class="line"></span><br><span class="line">    # ===========================================================================</span><br><span class="line">    # Use watermark to avoid frequent cache eviction.</span><br><span class="line">    # 决定是否能为当前seq分配物理块</span><br><span class="line">    # ===========================================================================</span><br><span class="line">    # 如果设备中所有的物理块数量 - 该seq实际需要的物理块数量 &lt; 水位线block数量，则不分配</span><br><span class="line">    # （说明当前seq太长了）</span><br><span class="line">    if (self.num_total_gpu_blocks - num_required_blocks &lt;</span><br><span class="line">            self.watermark_blocks):</span><br><span class="line">        return AllocStatus.NEVER</span><br><span class="line">    # 如果设备中可用的物理块数量 - 该seq实际需要的block数量 &gt;= 水位线block数量，则分配</span><br><span class="line">    if num_free_gpu_blocks - num_required_blocks &gt;= self.watermark_blocks:</span><br><span class="line">        return AllocStatus.OK</span><br><span class="line">    # 否则，现在不能分配，但可以延迟分配</span><br><span class="line">    else:</span><br><span class="line">        return AllocStatus.LATER</span><br></pre></td></tr></table></figure>

<p><strong>我们对上述代码做一些额外的说明：</strong></p>
<ul>
<li><p>代码第 32 行： <strong>num_free_gpu_blocks &#x3D; self.gpu_allocator.get_num_free_blocks()</strong> 。这里是在统计当前 gpu 上所有可用的物理块数数量（忘记 gpu_allocator 是什么的朋友，可以再回顾下 4.1 的调度器结构图）。在 vLLM 中，gpu_allocator 的类型有两种：</p>
<ul>
<li><strong>CachedBlockAllocator</strong> ： <strong>按照prefix caching的思想来分配和管理物理块</strong> 。在原理篇中，我们提过又些 prompts 中可能含有类似 system message（例如，“假设你是一个能提供帮助的行车导航”）E）等 prefix 信息，带有这些相同 prefix 信息的 prompt 完全可以共享用于存放 prefix 的物理块，这样既节省显存，也不用再对 prefix 做推理。</li>
<li><strong>UncachedBlockAllocator</strong> ： <strong>正常分配和管理物理块，没有额外实现prefix caching的功能</strong> 。</li>
</ul>
</li>
</ul>
<p><strong>关于这两种allocator的具体实现方式，我们将放在源码解读第3篇块管理来做讲解。这里大家只要明白大致定义即可，并不影响我们对调度策略的解读。</strong></p>
<ul>
<li><strong>self.watermark_blocks：水位线block数量，它起的是一个预警和缓冲的作用</strong> ，防止在 1 次调度中把 gpu 上预留给 KV Cache 的显存空间打得过满，出现一些意外风险（毕竟这个预留的显存空间也是我们估计出来的）。</li>
<li><strong>NEVER和LATER的区别</strong> ： <strong>这两者的相同之处在于，都是因为当前显存空间不够，而无法继续调度seq_group</strong> 。区别在于， <strong>NEVER是因为这条seq实在太长（即prompt太长），长到动用了gpu上所有的block（num_total_gpu_blocks）都无法处理它</strong> ，所以后续步骤中我们会直接把这个 seq 标记为完成，不再处理它；而 <strong>LATER是因为之前可能已经调度了很多seq_group，它们占据了相当一部分显存空间，导致gpu上剩余的可用block（num_free_gpu_blocks）无法再处理它</strong> ，所以我们延迟处理。</li>
</ul>
<h3 id="4-5-can-append-slot：能否为-seq-group-分配物理块做-decode"><a href="#4-5-can-append-slot：能否为-seq-group-分配物理块做-decode" class="headerlink" title="4.5 can_append_slot：能否为 seq_group 分配物理块做 decode"></a>4.5 can_append_slot：能否为 seq_group 分配物理块做 decode</h3><p><img src="/assets/image-20240806100742-kcs5cig.png" alt="image">​</p>
<p>回顾 4.2 调度器的流程图，你会看到我们从 running 队列中调度 seq_group 时，我们也会判断是否能为该 seq_group 分配物理块。<strong>但这时，我们的物理块空间是用来做decode的（给每个seq分配1个token的位置），而不是用来做prefill的（给每个seq分配若干个token的位置），所以这里我们采取的是另一种判断方法can_append_slot。</strong></p>
<p>更具体来说，running 队列中 seq_group 下的 n 个 seqs 在上 1 个推理阶段共生成了 n 个 token。在本次调度中，我们要先为这 n 个 token 分配物理块空间，用于存放它们在本次调度中即将产生的 KV 值。</p>
<p>好，我们再回到这个 seq_group 的 n 个 seqs 上来，我们知道：</p>
<ul>
<li><p>当往 1 个 seq 的物理块上添加 1 个 token 时，可能有两种情况：</p>
<ul>
<li>之前的物理块满了，所以我新开 1 个物理块给它</li>
<li>之前的物理块没满，我直接添加在最后一个物理块的空槽位上</li>
<li><strong>所以，对于1个seq来说，最坏的情况就是添加1个物理块；对于n个seqs来说，最坏的情况就是添加n个物理块（想想原理篇中讲过的copy-on-write机制）</strong></li>
</ul>
</li>
<li><p><strong>对于1个seq_group，除了那些标记为“finish”的seq外，其余seqs要么一起送去推理，要么一起不送去推理。即它们是集体行动的</strong></p>
</li>
</ul>
<p><strong>所以，判断能否对一个正在running的seq_group继续做推理的最保守的方式，就是判断当前可用的物理块数量是否至少为n。</strong></p>
<p>我们直接看代码（一切尽在注释中）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># vllm/core/block_manager_v1.py</span><br><span class="line">def can_append_slot(self, seq_group: SequenceGroup) -&gt; bool:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    对于这个seq_group，我们检查对于其中的每一个seq，</span><br><span class="line">    是否能至少分配一个空闲物理块给它</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Simple heuristic: If there is at least one free block</span><br><span class="line">    # for each sequence, we can append.</span><br><span class="line">    num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()</span><br><span class="line">    num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)</span><br><span class="line">    return num_seqs &lt;= num_free_gpu_blocks</span><br></pre></td></tr></table></figure>

<h3 id="4-6-allocate-与-append-slot：为-seq-group-分配物理块"><a href="#4-6-allocate-与-append-slot：为-seq-group-分配物理块" class="headerlink" title="4.6 allocate 与 append_slot：为 seq_group 分配物理块"></a>4.6 allocate 与 append_slot：为 seq_group 分配物理块</h3><p>当我们判断当前有充足的 gpu KV Cache 空间给对应的 seq_group 做新一轮推理时，我们就可以实际给它分配物理块了。这一块的内容涉及的细节太多（不同的 prefix caching 方式，逻辑块到物理块的映射，物理块释放，物理块的 refcount 即 copy-on-write 机制等等），所以我们把这部分留在源码解读 3：块管理中来详细说明。</p>
<p>跳过这块并不影响大家对调度器策略的解读。</p>
<h3 id="4-7-调度器核心代码"><a href="#4-7-调度器核心代码" class="headerlink" title="4.7 调度器核心代码"></a>4.7 调度器核心代码</h3><p>有了以上的基础（真是庞大的逻辑），我们现在终于能来看调度器中关于一次调度策略的核心代码了， <strong>大家可以配合4.2流程图阅读</strong> ，一切尽在注释中 ～</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br></pre></td><td class="code"><pre><span class="line"># vllm/core/scheduler.py</span><br><span class="line">def _schedule(self) -&gt; SchedulerOutputs:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">  </span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # blocks_to_swap_in：&#123;cpu物理块id: gpu物理块id&#125;</span><br><span class="line">    # blocks_to_swap_out：&#123;gpu物理块id: cpu物理块id&#125;</span><br><span class="line">    # blocks_to_copy: &#123;旧物理块id：[由旧物理块copy-on-write而来的新物理块id]&#125;</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    blocks_to_swap_in: Dict[int, int] = &#123;&#125;</span><br><span class="line">    blocks_to_swap_out: Dict[int, int] = &#123;&#125;</span><br><span class="line">    blocks_to_copy: Dict[int, List[int]] = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # Fix the current time.</span><br><span class="line">    # 获取当下时间</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    now = time.time()</span><br><span class="line"></span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # Join waiting sequences if possible.</span><br><span class="line">    # 如果swapped队列为空</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    if not self.swapped:</span><br><span class="line">        # ==========================================================================</span><br><span class="line">        # ignored_seq_groups：记录因太长（所需的blocks和总blocks之间的差值超过阈值了），</span><br><span class="line">        # 而无法继续做生成的seq_group，这些seq_group中的seq状态都会被标记为</span><br><span class="line">        # FINISHED_IGNORED，表示直接不处理他们</span><br><span class="line">        # ==========================================================================</span><br><span class="line">        ignored_seq_groups: List[SequenceGroup] = []</span><br><span class="line">      </span><br><span class="line">        # ==========================================================================</span><br><span class="line">        # 记录本次被调度的seq_group</span><br><span class="line">        # ==========================================================================</span><br><span class="line">        scheduled: List[SequenceGroup] = []</span><br><span class="line">      </span><br><span class="line">        # ==========================================================================</span><br><span class="line">        # The total number of sequences on the fly, including the</span><br><span class="line">        # requests in the generation phase.</span><br><span class="line">        # 计算Scheduler running队列中还没有执行完的seq数量</span><br><span class="line">        # ==========================================================================</span><br><span class="line">        num_curr_seqs = sum(seq_group.get_max_num_running_seqs()</span><br><span class="line">                            for seq_group in self.running)</span><br><span class="line">        curr_loras = set(</span><br><span class="line">            seq_group.lora_int_id</span><br><span class="line">            for seq_group in self.running) if self.lora_enabled else None</span><br><span class="line"></span><br><span class="line">        # ==========================================================================</span><br><span class="line">        # Optimization: We do not sort the waiting queue since the preempted</span><br><span class="line">        # sequence groups are added to the front and the new sequence groups</span><br><span class="line">        # are added to the back.</span><br><span class="line">        # lora相关的，可以暂时不看</span><br><span class="line">        # ==========================================================================</span><br><span class="line">        leftover_waiting_sequences = deque()</span><br><span class="line">      </span><br><span class="line">        # ==========================================================================</span><br><span class="line">        # 本次调度处理的token总数</span><br><span class="line">        # ==========================================================================</span><br><span class="line">        num_batched_tokens = 0</span><br><span class="line"></span><br><span class="line">        # ==========================================================================</span><br><span class="line">        # 开启新一次调度（while循环不结束意味着本次调度不结束，</span><br><span class="line">        # 跳出while循环时意味着本次调度结束了）</span><br><span class="line">        # 开启新一次调度的条件：当waiting队列中有等待处理的请求，且当前时刻可以处理请求</span><br><span class="line">        # ==========================================================================</span><br><span class="line">        while self._passed_delay(now) and self.waiting:</span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 取出waiting队列中的第一个请求，也即最早到达的请求（seq_group）</span><br><span class="line">            # =====================================================================</span><br><span class="line">            seq_group = self.waiting[0]</span><br><span class="line">          </span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 统计该seq_group中s处于waiting的seq的数量</span><br><span class="line">            # =====================================================================</span><br><span class="line">            waiting_seqs = seq_group.get_seqs(</span><br><span class="line">                status=SequenceStatus.WAITING)</span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 从waiting队列中取出来的seq_group，其seq数量一定是1</span><br><span class="line">            # =====================================================================</span><br><span class="line">            assert len(waiting_seqs) == 1, (</span><br><span class="line">                &quot;Waiting sequence group should have only one prompt &quot;</span><br><span class="line">                &quot;sequence.&quot;)</span><br><span class="line">          </span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 获取该seq的序列长度（如果该seq_group来自之前被抢占的请求，</span><br><span class="line">            # 那么这个长度不仅包括prompt，</span><br><span class="line">            # 还包括output）</span><br><span class="line">            # =====================================================================               </span><br><span class="line">            num_prefill_tokens = waiting_seqs[0].get_len()</span><br><span class="line">          </span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 如果从waiting队列中取出的这条seq的长度 &gt; 每次调度能处理的最大序列长度，</span><br><span class="line">            # 那么就打印警告信息，同时把这条seq的状态置为FINISHED_IGNORED，</span><br><span class="line">            # 并将对应seq_group装入ignored_seq_groups中，</span><br><span class="line">            # 然后将其从waiting列表中移除，不再处理</span><br><span class="line">            # =====================================================================</span><br><span class="line">            if num_prefill_tokens &gt; self.prompt_limit:</span><br><span class="line">                logger.warning(</span><br><span class="line">                    f&quot;Input prompt (&#123;num_prefill_tokens&#125; tokens) is too &quot;</span><br><span class="line">                    f&quot;long and exceeds limit of &#123;self.prompt_limit&#125;&quot;)</span><br><span class="line">                for seq in waiting_seqs:</span><br><span class="line">                    seq.status = SequenceStatus.FINISHED_IGNORED</span><br><span class="line">                ignored_seq_groups.append(seq_group)</span><br><span class="line">                self.waiting.popleft()</span><br><span class="line">                continue</span><br><span class="line">          </span><br><span class="line">            # =====================================================================</span><br><span class="line">            # If the sequence group cannot be allocated, stop.</span><br><span class="line">            # 决定是否能给当前seq_group分配物理块</span><br><span class="line">            # can_allocate返回值可能有三种：</span><br><span class="line">            #       AllocStatus.NEVER：不分配；</span><br><span class="line">            #       AllocStatus.OK：可以分配；</span><br><span class="line">            #       AllocStatus.LATER：延迟分配</span><br><span class="line">            # =====================================================================</span><br><span class="line">            can_allocate = self.block_manager.can_allocate(seq_group)</span><br><span class="line">            # 若是延迟分配，则说明现在没有足够的block空间，所以跳出while循环（不继续对waiting队列中的数据做处理了）</span><br><span class="line">            if can_allocate == AllocStatus.LATER:</span><br><span class="line">                break</span><br><span class="line">            # 如果不分配，说明seq长得超出了vLLM的处理范围，则后续也不再处理它，直接将该seq状态标记为FINISHED_IGNORED</span><br><span class="line">            elif can_allocate == AllocStatus.NEVER:</span><br><span class="line">                logger.warning(</span><br><span class="line">                    f&quot;Input prompt (&#123;num_prefill_tokens&#125; tokens) is too &quot;</span><br><span class="line">                    f&quot;long and exceeds the capacity of block_manager&quot;)</span><br><span class="line">                for seq in waiting_seqs:</span><br><span class="line">                    seq.status = SequenceStatus.FINISHED_IGNORED</span><br><span class="line">                ignored_seq_groups.append(seq_group) # 记录因为太长而无法处理的seq_group</span><br><span class="line">                self.waiting.popleft() # 将该seq_group从waiting队列中移除</span><br><span class="line">                continue</span><br><span class="line"></span><br><span class="line">            # =====================================================================                 # lora推理相关的部分，可暂时忽略</span><br><span class="line">            # =====================================================================</span><br><span class="line">            lora_int_id = 0</span><br><span class="line">            if self.lora_enabled:</span><br><span class="line">                lora_int_id = seq_group.lora_int_id</span><br><span class="line">                if (lora_int_id &gt; 0 and lora_int_id not in curr_loras</span><br><span class="line">                        and len(curr_loras) &gt;= self.lora_config.max_loras):</span><br><span class="line">                    # We don&#x27;t have a space for another LoRA, so</span><br><span class="line">                    # we ignore this request for now.</span><br><span class="line">                    leftover_waiting_sequences.appendleft(seq_group)</span><br><span class="line">                    self.waiting.popleft()</span><br><span class="line">                    continue</span><br><span class="line"></span><br><span class="line">            # =====================================================================</span><br><span class="line">            # If the number of batched tokens exceeds the limit, stop.</span><br><span class="line">            # max_num_batched_tokens：单次调度中最多处理的token数量</span><br><span class="line">            # num_batched_tokens：本次调度中累计处理的token数量</span><br><span class="line">            # 如果后者 &gt; 前者，则结束本次调度</span><br><span class="line">            # =====================================================================</span><br><span class="line">            num_batched_tokens += num_prefill_tokens</span><br><span class="line">            if (num_batched_tokens &gt;</span><br><span class="line">                    self.scheduler_config.max_num_batched_tokens):</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">            # =====================================================================</span><br><span class="line">            # The total number of sequences in the RUNNING state should not</span><br><span class="line">            # exceed the maximum number of sequences.</span><br><span class="line">            # num_new_seqs: 当前seq_group中状态为“未执行完”的序列的数量</span><br><span class="line">            # num_curr_seqs：当前调度轮次中，状态为&quot;未执行完“的序列总数</span><br><span class="line">            # 如果超过了我们对单次调度能执行的序列总数的阈值，就结束本次调度</span><br><span class="line">            # =====================================================================</span><br><span class="line">            num_new_seqs = seq_group.get_max_num_running_seqs()</span><br><span class="line">            if (num_curr_seqs + num_new_seqs &gt;</span><br><span class="line">                    self.scheduler_config.max_num_seqs): # 单次迭代中最多处理多少个序列</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">            if lora_int_id &gt; 0:</span><br><span class="line">                curr_loras.add(lora_int_id)</span><br><span class="line">          </span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 走到这一步时，说明当前seq_group已经通过上述种种验证，可以被加入本次调度中执行了</span><br><span class="line">            # 先将其从waiting队列中移出</span><br><span class="line">            # =====================================================================</span><br><span class="line">            self.waiting.popleft()</span><br><span class="line">          </span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 为当前seq_group分配物理块</span><br><span class="line">            # =====================================================================</span><br><span class="line">            self._allocate(seq_group)</span><br><span class="line">          </span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 将当前seq_group放入running队列中</span><br><span class="line">            # =====================================================================</span><br><span class="line">            self.running.append(seq_group)</span><br><span class="line">          </span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 记录本次调度累计处理的序列数量</span><br><span class="line">            # =====================================================================</span><br><span class="line">            num_curr_seqs += num_new_seqs</span><br><span class="line">          </span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 记录本次被调度的seq_group</span><br><span class="line">            # =====================================================================</span><br><span class="line">            scheduled.append(</span><br><span class="line">                ScheduledSequenceGroup(</span><br><span class="line">                    seq_group=seq_group,</span><br><span class="line">                    token_chunk_size=num_prefill_tokens))</span><br><span class="line">      </span><br><span class="line">        # =====================================================================</span><br><span class="line">        # 和lora相关的操作，暂时忽略</span><br><span class="line">        # =====================================================================</span><br><span class="line">        self.waiting.extendleft(leftover_waiting_sequences)</span><br><span class="line"></span><br><span class="line">        # =====================================================================</span><br><span class="line">        # 如果本次有被调度的seq_group（scheduled非空）</span><br><span class="line">        # 或者本次有被设置为不再处理的seq_group(ignored_seq_groups非空)</span><br><span class="line">        # 就将其包装成SchedulerOutputs对象</span><br><span class="line">        # =====================================================================</span><br><span class="line">        if scheduled or ignored_seq_groups:</span><br><span class="line">            self.prev_prompt = True</span><br><span class="line">            scheduler_outputs = SchedulerOutputs(</span><br><span class="line">                scheduled_seq_groups=scheduled,</span><br><span class="line">                prompt_run=True,</span><br><span class="line">                num_batched_tokens=num_batched_tokens,</span><br><span class="line">                blocks_to_swap_in=blocks_to_swap_in,</span><br><span class="line">                blocks_to_swap_out=blocks_to_swap_out,</span><br><span class="line">                blocks_to_copy=blocks_to_copy,</span><br><span class="line">                ignored_seq_groups=ignored_seq_groups,</span><br><span class="line">            )</span><br><span class="line">            return scheduler_outputs</span><br><span class="line"></span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # NOTE(woosuk): Preemption happens only when there is no available slot</span><br><span class="line">    # to keep all the sequence groups in the RUNNING state.</span><br><span class="line">    # In this case, the policy is responsible for deciding which sequence</span><br><span class="line">    # groups to preempt.</span><br><span class="line">    # 如果swap队列非空，且本次没有新的需要被发起推理的seq_group,</span><br><span class="line">    # 则对running队列中的seq_group，</span><br><span class="line">    # 按照 &quot;当前时间-该seq_group到达时间&quot; ，从早到晚排列running队列中的seq_group</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    self.running = self.policy.sort_by_priority(now, self.running)</span><br><span class="line"></span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # Reserve new token slots for the running sequence groups.</span><br><span class="line">    # 初始化一个新的running队列（deque（））</span><br><span class="line">    # 初始化一个抢占列表</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    running: Deque[SequenceGroup] = deque()</span><br><span class="line">    preempted: List[SequenceGroup] = []</span><br><span class="line">  </span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # 当running队列非空时</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    while self.running:</span><br><span class="line">        # 取出running队列中最早到来的seq_group</span><br><span class="line">        seq_group = self.running.popleft()</span><br><span class="line">        # =====================================================================</span><br><span class="line">        # 对于running队列中这个最早到来的seq_group，检查对于其中的每一个seq，</span><br><span class="line">        # 是否能至少分配一个物理块给它，如果不能的话</span><br><span class="line">        # （说明要执行抢占操作了，否则马上会没有资源让这个最早到达的seq_group做完推理）：</span><br><span class="line">        # （注意，这里用了while...else，如果while条件正常结束，则进入else内容；</span><br><span class="line">        #  如果被break，则不会执行else）</span><br><span class="line">        # =====================================================================</span><br><span class="line">        while not self.block_manager.can_append_slot(seq_group):</span><br><span class="line">            # =====================================================================</span><br><span class="line">            # 如果从running队列中取出最早达到的seq_group后，running队列还是非空</span><br><span class="line">            # =====================================================================</span><br><span class="line">            if self.running:</span><br><span class="line">                # ==============================================================</span><br><span class="line">                # 抢占running队列中最晚到来的seq_group（可怜的被害者）</span><br><span class="line">                # ==============================================================</span><br><span class="line">                victim_seq_group = self.running.pop()</span><br><span class="line">              </span><br><span class="line">                # ==============================================================</span><br><span class="line">                # 一个seq_group被抢占后，有2中处理方式：</span><br><span class="line">                # - 如果该seq_group下只有一个seq，执行【重计算】，</span><br><span class="line">                #   将其从running队列中移除，并清空它的物理块，</span><br><span class="line">                #   将其seq的状态从running-&gt;waiting，并加入waiting队列。后面将重新计算</span><br><span class="line">                #</span><br><span class="line">                # - 如果该seq_group下有多个seq，执行【swap】，</span><br><span class="line">                #   清空它的gpu物理块，并为这些物理块做好cpu物理块映射，</span><br><span class="line">                #   这些seq的block_table字典中（&#123;seq_id: block_table&#125;）的block_table</span><br><span class="line">                #   从gpu物理块改成cpu物理块</span><br><span class="line">                #   将其seqs状态从running -&gt; swapped，加入swapped队列</span><br><span class="line">                # ==============================================================</span><br><span class="line">                self._preempt(victim_seq_group, blocks_to_swap_out)</span><br><span class="line">                preempted.append(victim_seq_group)</span><br><span class="line">            # ==============================================================</span><br><span class="line">            # 如果除这个最早到来的seq_group外，running队列中再没有别的seq_group了，</span><br><span class="line">            # 且此时又没有足够的空间留给这个最早来的seq_group做推理了，那么只能抢占它</span><br><span class="line">            # ==============================================================</span><br><span class="line">            else:</span><br><span class="line">                # 那就只能抢占这个最早到达的seq_group了</span><br><span class="line">                # No other sequence groups can be preempted.</span><br><span class="line">                # Preempt the current sequence group.</span><br><span class="line">                self._preempt(seq_group, blocks_to_swap_out)</span><br><span class="line">                preempted.append(seq_group)</span><br><span class="line">                break</span><br><span class="line">        # ==============================================================</span><br><span class="line">        # 如果此时有足够的空间给running队列中最早来的seq_group做推理了</span><br><span class="line">        # ==============================================================</span><br><span class="line">        else:</span><br><span class="line">            # ==============================================================</span><br><span class="line">            #  Append new slots to the sequence group.</span><br><span class="line">            # seq_group里的每个seq正常做推理。假设现在每个seq正常生成一个token，我们需要根据每个seq当前</span><br><span class="line">            # 维护的最后一个物理块的情况，决定是否需要分配新的物理块，决定的结果可能如下：</span><br><span class="line">            # - 物理块refcount = 1，且有充足槽位，则无需分配新物理块</span><br><span class="line">            # - 物理块refcount = 1，且无充足槽位，分配新的物理块</span><br><span class="line">            # - 物理块refcount &gt; 1, 采用copy-on-write机制，分配新物理块，对该seq，</span><br><span class="line">            #                      用新物理块替换掉其block_table中维护的最后一个物理块</span><br><span class="line">            #                     （称为旧物理块）。释放旧物理块（令其refcount-1）。</span><br><span class="line">            #                      同时记录下新旧物理块之间的映射，</span><br><span class="line">            #   blocks_to_copy：&#123;旧物理块id：[由旧物理块copy-on-write而来的新物理块id]&#125;</span><br><span class="line">            # ==============================================================</span><br><span class="line">            self._append_slot(seq_group, blocks_to_copy)</span><br><span class="line">          </span><br><span class="line">            # ==============================================================</span><br><span class="line">            # 自定义的running队列中添加这个seq_group</span><br><span class="line">            # ==============================================================</span><br><span class="line">            running.append(seq_group)</span><br><span class="line">  </span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # 最终还能在running队列中运行的seq_group</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    self.running = running</span><br><span class="line"></span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # Swap in the sequence groups in the SWAPPED state if possible.</span><br><span class="line">    # 对于swapped队列中的seq_group，按照到达时间从早到晚排序</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    self.swapped = self.policy.sort_by_priority(now, self.swapped)</span><br><span class="line">  </span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # 如果本次调度没有新安排的被抢占的seq_group(即preempted为空)</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    if not preempted:</span><br><span class="line">        # ==============================================================</span><br><span class="line">        # 计算running队列中，所有seq_group下，“到生命周期结束为止最多运行的seq数量”的总和</span><br><span class="line">        # ==============================================================</span><br><span class="line">        num_curr_seqs = sum(seq_group.get_max_num_running_seqs()</span><br><span class="line">                            for seq_group in self.running)</span><br><span class="line">        # ==============================================================</span><br><span class="line">        # lora部分，暂时忽略</span><br><span class="line">        # ==============================================================</span><br><span class="line">        curr_loras = set(</span><br><span class="line">            seq_group.lora_int_id</span><br><span class="line">            for seq_group in self.running) if self.lora_enabled else None</span><br><span class="line"></span><br><span class="line">        # ==============================================================</span><br><span class="line">        # lora相关的，可以暂时不看</span><br><span class="line">        # ==============================================================</span><br><span class="line">        leftover_swapped = deque()</span><br><span class="line"></span><br><span class="line">        # ==============================================================</span><br><span class="line">        # 当swapped队列非空时</span><br><span class="line">        # ==============================================================</span><br><span class="line">        while self.swapped:</span><br><span class="line">            # ==============================================================</span><br><span class="line">            # 取出swap队列中最早被抢占的seq_group</span><br><span class="line">            # ==============================================================</span><br><span class="line">            seq_group = self.swapped[0]</span><br><span class="line">            # ==============================================================</span><br><span class="line">            # lora相关，暂时不看</span><br><span class="line">            # ==============================================================</span><br><span class="line">            lora_int_id = 0</span><br><span class="line">            if self.lora_enabled:</span><br><span class="line">                lora_int_id = seq_group.lora_int_id</span><br><span class="line">                if (lora_int_id &gt; 0 and lora_int_id not in curr_loras</span><br><span class="line">                        and len(curr_loras) &gt;= self.lora_config.max_loras):</span><br><span class="line">                    # We don&#x27;t have a space for another LoRA, so</span><br><span class="line">                    # we ignore this request for now.</span><br><span class="line">                    leftover_swapped.appendleft(seq_group)</span><br><span class="line">                    self.swapped.popleft()</span><br><span class="line">                    continue</span><br><span class="line"></span><br><span class="line">            # ==============================================================</span><br><span class="line">            # If the sequence group cannot be swapped in, stop.</span><br><span class="line">            # 判断一个被swap的seq_group现在是否能重新running起来</span><br><span class="line">            # 【判断条件】：</span><br><span class="line">            # 当前gpu上可用的物理块数量 - 重新跑起这个seq_group需要的物理块数量 </span><br><span class="line">            # &gt;= 水位线物理块数量</span><br><span class="line">            # 其中：</span><br><span class="line">            # 后者 = 在被swap之前它已经使用的物理块数量（去重过了） </span><br><span class="line">            #       + 若能再次跑起来它至少需要的物理块数量</span><br><span class="line">            #（假设每个seq至少需要1个物理块）</span><br><span class="line">            # ==============================================================</span><br><span class="line">            # 如果不能，则意味着当前没有充足资源处理swap队列中的seq_group，则直接跳出循环</span><br><span class="line">            if not self.block_manager.can_swap_in(seq_group):</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">            # ==============================================================</span><br><span class="line">            # The total number of sequences in the RUNNING state should not</span><br><span class="line">            # exceed the maximum number of sequences.</span><br><span class="line">            # 如果对于swap队列中的这个seq_group，当前gpu上有充足资源可以让它重新跑起来的话：</span><br><span class="line">            # ==============================================================</span><br><span class="line">            # 取出这个seq_group在剩余生命周期内将并行运行的最大序列数</span><br><span class="line">            num_new_seqs = seq_group.get_max_num_running_seqs()</span><br><span class="line">            # 如果已超过一次调度中能处理的最大序列数，则不再对该seq_group进行处理</span><br><span class="line">            if (num_curr_seqs + num_new_seqs &gt;</span><br><span class="line">                    self.scheduler_config.max_num_seqs):</span><br><span class="line">                break</span><br><span class="line">          </span><br><span class="line">            # lora部分暂时不看</span><br><span class="line">            if lora_int_id &gt; 0:</span><br><span class="line">                curr_loras.add(lora_int_id)</span><br><span class="line">          </span><br><span class="line">            # ==============================================================</span><br><span class="line">            # 走到这一步，说明可以对swapped队列中的这个seq_group做相关处理了，</span><br><span class="line">            # 先把它从队列中移出去</span><br><span class="line">            # ==============================================================</span><br><span class="line">            self.swapped.popleft()</span><br><span class="line">          </span><br><span class="line">            # ==============================================================</span><br><span class="line">            # 将该seq_group下所有cpu块置换回gpu块，</span><br><span class="line">            # 并将其下每个seq的状态从swapped改成running</span><br><span class="line">            # ==============================================================</span><br><span class="line">            self._swap_in(seq_group, blocks_to_swap_in)</span><br><span class="line">          </span><br><span class="line">            # ==============================================================</span><br><span class="line">            # 假设其正常做推理了，假设现在生成了一个token，要如何分配物理块（参见上面注释）</span><br><span class="line">            # ==============================================================</span><br><span class="line">            self._append_slot(seq_group, blocks_to_copy)</span><br><span class="line">            num_curr_seqs += num_new_seqs</span><br><span class="line">            self.running.append(seq_group)</span><br><span class="line"></span><br><span class="line">        self.swapped.extendleft(leftover_swapped)</span><br><span class="line"></span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # 如果本次调度有新安排的被抢占的seq_group(即preempted不为空)，那就准备将最终的running队列</span><br><span class="line">    # 作为scheduleroutputs返回</span><br><span class="line">    # ==============================================================================</span><br><span class="line">  </span><br><span class="line">    # Each sequence in the generation phase only takes one token slot.</span><br><span class="line">    # Therefore, the number of batched tokens is equal to the number of</span><br><span class="line">    # sequences in the RUNNING state.</span><br><span class="line">    # 由于每个seq一次只生成1个token，因此num_batched_tokens = 状态为running的seq数量</span><br><span class="line">    num_batched_tokens = sum(</span><br><span class="line">        seq_group.num_seqs(status=SequenceStatus.RUNNING)</span><br><span class="line">        for seq_group in self.running)</span><br><span class="line"></span><br><span class="line">    # ==============================================================================</span><br><span class="line">    # 构建Schduleroutputs</span><br><span class="line">    # ==============================================================================</span><br><span class="line">    scheduler_outputs = SchedulerOutputs(</span><br><span class="line">        scheduled_seq_groups=[</span><br><span class="line">            ScheduledSequenceGroup(seq_group=running_group,</span><br><span class="line">                                   token_chunk_size=1)</span><br><span class="line">            for running_group in self.running</span><br><span class="line">        ],</span><br><span class="line">        prompt_run=False,</span><br><span class="line">        num_batched_tokens=num_batched_tokens,</span><br><span class="line">        blocks_to_swap_in=blocks_to_swap_in,</span><br><span class="line">        blocks_to_swap_out=blocks_to_swap_out,</span><br><span class="line">        blocks_to_copy=blocks_to_copy,</span><br><span class="line">        ignored_seq_groups=[],</span><br><span class="line">    )</span><br><span class="line">    return scheduler_outputs</span><br></pre></td></tr></table></figure>

<h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>在本文中，我们：</p>
<ul>
<li><strong>从vLLM批处理的入口函数开始，介绍了其推理内核LLMEngine的两个重要函数add_request()和step()</strong></li>
<li><strong>在LLMEngine开始处理请求前（实例化阶段），它会先做一次模拟实验，来估计gpu上需要预留多少显存给KV Cache block。</strong></li>
<li><strong>当LLMEngine开始处理请求时(add_request)，它会把每个prompt当成一个请求，同时把它包装成一个SequenceGroup对象。</strong></li>
<li><strong>当LLMEngine开始执行1次调度时（step），调度器策略(Scheduler)会根据实际gpu上KV Cache block的使用情况等要素，来选择要送哪些seq_group去做新一轮推理。注意，在1次推理中，所有seq_group要么一起做prefill，要么一起做decode。</strong></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://wbice.cn">weibingo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://wbice.cn/article/vLLM%E5%BC%95%E6%93%8E%E8%A7%A3%E6%9E%90-%E8%B0%83%E5%BA%A6%E5%88%86%E6%9E%90.html">https://wbice.cn/article/vLLM%E5%BC%95%E6%93%8E%E8%A7%A3%E6%9E%90-%E8%B0%83%E5%BA%A6%E5%88%86%E6%9E%90.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wbice.cn" target="_blank">WBINGのBLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></div><div class="post_share"><div class="social-share" data-image="https://hexo-1256892004.cos.ap-beijing.myqcloud.com/page/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/article/CUDA%E5%AD%A6%E4%B9%A0%E4%B9%8Bhello%20world.html" title="CUDA学习之hello world"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CUDA学习之hello world</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/article/vLLM%E5%BC%95%E6%93%8E%E8%A7%A3%E6%9E%90-%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88.html" title="LLM引擎解析-框架概览"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-12</div><div class="title">LLM引擎解析-框架概览</div></div></a></div><div><a href="/article/%E9%A3%9E%E6%A1%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF.html" title="飞桨大模型分布式训练技术"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-09</div><div class="title">飞桨大模型分布式训练技术</div></div></a></div><div><a href="/article/%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%E5%9C%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E7%9A%84%E6%8A%80%E6%9C%AF%E5%92%8C%E5%AE%9E%E8%B7%B5.html" title="向量检索在大模型应用场景的技术和实践"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-22</div><div class="title">向量检索在大模型应用场景的技术和实践</div></div></a></div><div><a href="/article/LMOps%E5%B7%A5%E5%85%B7%E9%93%BE%E4%B8%8E%E5%8D%83%E5%B8%86%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%B3%E5%8F%B0.html" title="LMOps工具链与千帆大模型平台"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-22</div><div class="title">LMOps工具链与千帆大模型平台</div></div></a></div><div><a href="/article/%E5%A4%A7%E8%A7%84%E6%A8%A1AI%E9%AB%98%E6%80%A7%E8%83%BD%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E8%B7%B5.html" title="大规模AI高性能网络的设计与实践"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-21</div><div class="title">大规模AI高性能网络的设计与实践</div></div></a></div><div><a href="/article/GPT%E5%92%8CBERT%E7%9A%84%E5%B7%AE%E5%88%AB.html" title="GPT和BERT的差别"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-02</div><div class="title">GPT和BERT的差别</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://hexo-1256892004.cos.ap-beijing.myqcloud.com/page/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">weibingo</div><div class="author-info__description">AI探索者,经济迷,浅度摄影,爱好历史</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">55</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/weibingo"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/weibingo" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:953952016@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81vLLM-%E7%9A%84%E8%B0%83%E7%94%A8%E6%96%B9%E5%BC%8F"><span class="toc-number">1.</span> <span class="toc-text">一、vLLM 的调用方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Offline-Batched-Inference"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 Offline Batched Inference</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-API-Server-For-Online-Serving"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 API Server For Online Serving</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%80%BB%E7%BB%93"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81vLLM-%E4%BB%A3%E7%A0%81%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="toc-number">2.</span> <span class="toc-text">二、vLLM 代码整体架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Centralized-Controller"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Centralized Controller</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">2.2.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Distributed-Workers"><span class="toc-number">2.3.</span> <span class="toc-text">2.2 Distributed Workers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%A2%84%E5%88%86%E9%85%8D%E6%98%BE%E5%AD%98"><span class="toc-number">3.</span> <span class="toc-text">三、加载模型与预分配显存</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 加载模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%A2%84%E5%88%86%E9%85%8D%E6%98%BE%E5%AD%98"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 预分配显存</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Scheduler-%E8%B0%83%E5%BA%A6"><span class="toc-number">4.</span> <span class="toc-text">四、Scheduler 调度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81SequenceGroup"><span class="toc-number">4.1.</span> <span class="toc-text">一、SequenceGroup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%8E%9F%E7%94%9F%E8%BE%93%E5%85%A5"><span class="toc-number">4.2.</span> <span class="toc-text">3.1 原生输入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-SequenceGroup-%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">4.3.</span> <span class="toc-text">3.2 SequenceGroup 的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-SequenceGroup-%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">4.4.</span> <span class="toc-text">3.3 SequenceGroup 的结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81add-request-%EF%BC%9A%E5%B0%86-seq-group-%E6%B7%BB%E5%8A%A0%E8%BF%9B%E8%B0%83%E5%BA%A6%E5%99%A8-waiting-%E9%98%9F%E5%88%97"><span class="toc-number">5.</span> <span class="toc-text">三、add_request()：将 seq_group 添加进调度器 waiting 队列</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%EF%BC%9Astep-%EF%BC%9A%E8%B0%83%E5%BA%A6%E5%99%A8%E7%AD%96%E7%95%A5"><span class="toc-number">6.</span> <span class="toc-text">四：step()：调度器策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E8%B0%83%E5%BA%A6%E5%99%A8%E7%BB%93%E6%9E%84"><span class="toc-number">6.1.</span> <span class="toc-text">4.1 调度器结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%95%B4%E4%BD%93%E8%B0%83%E5%BA%A6%E6%B5%81%E7%A8%8B"><span class="toc-number">6.2.</span> <span class="toc-text">4.2 整体调度流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-passed-delay%EF%BC%9A%E5%88%A4%E6%96%AD%E8%B0%83%E5%BA%A6-waiting-%E9%98%9F%E5%88%97%E7%9A%84%E6%97%B6%E9%97%B4%E7%82%B9"><span class="toc-number">6.3.</span> <span class="toc-text">4.3 _passed_delay：判断调度 waiting 队列的时间点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-can-allocate%EF%BC%9A%E8%83%BD%E5%90%A6%E4%B8%BA-seq-group-%E5%88%86%E9%85%8D%E7%89%A9%E7%90%86%E5%9D%97%E5%81%9A-prefill"><span class="toc-number">6.4.</span> <span class="toc-text">4.4 can_allocate：能否为 seq_group 分配物理块做 prefill</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-can-append-slot%EF%BC%9A%E8%83%BD%E5%90%A6%E4%B8%BA-seq-group-%E5%88%86%E9%85%8D%E7%89%A9%E7%90%86%E5%9D%97%E5%81%9A-decode"><span class="toc-number">6.5.</span> <span class="toc-text">4.5 can_append_slot：能否为 seq_group 分配物理块做 decode</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-allocate-%E4%B8%8E-append-slot%EF%BC%9A%E4%B8%BA-seq-group-%E5%88%86%E9%85%8D%E7%89%A9%E7%90%86%E5%9D%97"><span class="toc-number">6.6.</span> <span class="toc-text">4.6 allocate 与 append_slot：为 seq_group 分配物理块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-%E8%B0%83%E5%BA%A6%E5%99%A8%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-number">6.7.</span> <span class="toc-text">4.7 调度器核心代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-number">7.</span> <span class="toc-text">五、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/article/vLLM%E5%BC%95%E6%93%8E%E8%A7%A3%E6%9E%90-%E8%B0%83%E5%BA%A6%E5%88%86%E6%9E%90.html" title="vLLM引擎解析-调度分析">vLLM引擎解析-调度分析</a><time datetime="2024-08-25T11:13:30.000Z" title="发表于 2024-08-25 19:13:30">2024-08-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/article/CUDA%E5%AD%A6%E4%B9%A0%E4%B9%8Bhello%20world.html" title="CUDA学习之hello world">CUDA学习之hello world</a><time datetime="2024-08-20T14:11:31.000Z" title="发表于 2024-08-20 22:11:31">2024-08-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/article/vLLM%E5%BC%95%E6%93%8E%E8%A7%A3%E6%9E%90-%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88.html" title="LLM引擎解析-框架概览">LLM引擎解析-框架概览</a><time datetime="2024-08-12T03:15:30.000Z" title="发表于 2024-08-12 11:15:30">2024-08-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/article/GRPC%E5%BA%95%E5%B1%82%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE-HTTP%20Trailer.html" title="GRPC底层传输协议-HTTP Trailer">GRPC底层传输协议-HTTP Trailer</a><time datetime="2024-03-15T07:15:30.000Z" title="发表于 2024-03-15 15:15:30">2024-03-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/article/GRPC%E5%BA%95%E5%B1%82%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE.html" title="GRPC底层传输协议">GRPC底层传输协议</a><time datetime="2024-03-12T03:11:30.000Z" title="发表于 2024-03-12 11:11:30">2024-03-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By weibingo</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'dEfDWKbVoLq95IOjY2Wucz01-9Nh9j0Va',
      appKey: 'CWRYxwiLTYcAG0OcWmKl7Ez0',
      avatar: 'monsterid',
      serverURLs: 'https://defdwkbv.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.22.1/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4.65.0/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js?v=4.13.0"></script></div></div></body></html>