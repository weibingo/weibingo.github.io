<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="spark加速Bulkload"><meta name="keywords" content="spark,hbase"><meta name="author" content="weibingo"><meta name="copyright" content="weibingo"><title>spark加速Bulkload | WBINGのBLOG</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.8.0"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.8.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"3FYJCLGPHO","apiKey":"d730ef95ff5ff79a19b74363531c066b","indexName":"prod_weibing-blog","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 6.3.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bulkload%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">Bulkload实现过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8MR%E7%94%9F%E6%88%90HFile"><span class="toc-number">2.1.</span> <span class="toc-text">利用MR生成HFile</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87BlukLoad%E6%96%B9%E5%BC%8F%E5%8A%A0%E8%BD%BDHFile%E6%96%87%E4%BB%B6"><span class="toc-number">2.2.</span> <span class="toc-text">通过BlukLoad方式加载HFile文件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8spark%E8%BF%9B%E8%A1%8CBulkload"><span class="toc-number">3.</span> <span class="toc-text">使用spark进行Bulkload</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-Bulkload%E7%9A%84%E4%B8%80%E4%BA%9B%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">4.</span> <span class="toc-text">spark Bulkload的一些注意事项</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kerberos%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98"><span class="toc-number">4.1.</span> <span class="toc-text">kerberos权限问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AEsnappy%E5%8E%8B%E7%BC%A9%E9%97%AE%E9%A2%98"><span class="toc-number">4.2.</span> <span class="toc-text">数据snappy压缩问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%8E%E7%BB%AD"><span class="toc-number">5.</span> <span class="toc-text">后续</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://hexo-1256892004.cos.ap-beijing.myqcloud.com/page/avatar.jpg"></div><div class="author-info__name text-center">weibingo</div><div class="author-info__description text-center">BIG DATA探索者,经济迷,浅度摄影,爱好历史,对社会意识,人文感兴趣</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/weibingo">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">45</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">42</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">19</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://hexo-1256892004.cos.ap-beijing.myqcloud.com/page/top.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">WBINGのBLOG</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">spark加速Bulkload</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-06-12</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B/">数据工程</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.7k</span><span class="post-meta__separator">|</span><span>Reading time: 6 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>本文介绍HBase在大数据量导入时Bulkload的操作过程，以及使用spark加速整个Bulkload。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在第一次建立Hbase表的时候，我们可能需要往里面一次性导入大量的初始化数据。我们很自然地想到将数据一条条插入到Hbase中，或者通过MR方式等。但是这些方式不是慢就是在导入的过程的占用Region资源导致效率低下，所以很不适合一次性导入大量数据。使用 Bulk Load 方式由于利用了 HBase 的数据信息是按照特定格式存储在 HDFS 里的这一特性，直接在 HDFS 中生成持久化的 HFile 数据格式文件，然后完成巨量数据快速入库的操作，配合 MapReduce 完成这样的操作，不占用 Region 资源，不会产生巨量的写入 I&#x2F;O，所以需要较少的 CPU 和网络资源。</p>
<h2 id="Bulkload实现过程"><a href="#Bulkload实现过程" class="headerlink" title="Bulkload实现过程"></a>Bulkload实现过程</h2><h3 id="利用MR生成HFile"><a href="#利用MR生成HFile" class="headerlink" title="利用MR生成HFile"></a>利用MR生成HFile</h3><p>因为HBase底层存储结构是HFile，而Hbase API为我们提供了生成HFile，我们只需要按照要求，写Mapper，生成特定的(ImmutableBytesWritable, Put)对，或者(ImmutableBytesWritable, KeyValue)即可。</p>
<span id="more"></span>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">BulkLoadMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt;&#123;</span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line">            String[] items = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">  </span><br><span class="line">            <span class="type">ImmutableBytesWritable</span> <span class="variable">rowKey</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ImmutableBytesWritable</span>(items[<span class="number">0</span>].getBytes());</span><br><span class="line">            <span class="type">Put</span> <span class="variable">put</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Put</span>(Bytes.toBytes(items[<span class="number">0</span>]));   <span class="comment">//ROWKEY</span></span><br><span class="line">            put.addColumn(<span class="string">&quot;f1&quot;</span>.getBytes(), <span class="string">&quot;url&quot;</span>.getBytes(), items[<span class="number">1</span>].getBytes());</span><br><span class="line">            put.addColumn(<span class="string">&quot;f1&quot;</span>.getBytes(), <span class="string">&quot;name&quot;</span>.getBytes(), items[<span class="number">2</span>].getBytes());</span><br><span class="line">            </span><br><span class="line">            context.write(rowkey, put);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Mapper中的KEYIN, VALUEIN根据你的文件格式来指定读取类。</p>
<p>MR驱动程序类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">BulkLoadDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">        <span class="keyword">final</span> String SRC_PATH= <span class="string">&quot;hdfs://input&quot;</span>;</span><br><span class="line">        <span class="keyword">final</span> String DESC_PATH= <span class="string">&quot;hdfs://output&quot;</span>;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> HBaseConfiguration.create();</span><br><span class="line">       </span><br><span class="line">        Job job=Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(BulkLoadDriver.class);</span><br><span class="line">        job.setMapperClass(BulkLoadMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">        <span class="comment">/** 根据VALUEOUT 类别指定Put 或者 KeyValue</span></span><br><span class="line"><span class="comment">         *框架会自行根据 MapOutputValueClass 来决定是使用 KeyValueSortReducer 还是 PutSortReducer</span></span><br><span class="line"><span class="comment">        **/</span></span><br><span class="line">        job.setMapOutputValueClass(Put.class);  </span><br><span class="line">        job.setOutputFormatClass(HFileOutputFormat2.class);</span><br><span class="line">        <span class="type">HTable</span> <span class="variable">table</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">HTable</span>(conf,<span class="string">&quot;user_tag&quot;</span>);</span><br><span class="line">        HFileOutputFormat2.configureIncrementalLoad(job,table,table.getRegionLocator());</span><br><span class="line">        FileInputFormat.addInputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(SRC_PATH));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> <span class="title class_">Path</span>(DESC_PATH));</span><br><span class="line">          </span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="通过BlukLoad方式加载HFile文件"><a href="#通过BlukLoad方式加载HFile文件" class="headerlink" title="通过BlukLoad方式加载HFile文件"></a>通过BlukLoad方式加载HFile文件</h3><p>生成的HFile还需要通过LoadIncrementalHFiles类的doBulkLoad方法，将HFile加载入hbase的目录下。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> HBaseConfiguration.create();</span><br><span class="line"><span class="type">Connection</span> <span class="variable">connection</span> <span class="operator">=</span> ConnectionFactory.createConnection(configuration);</span><br><span class="line"><span class="type">LoadIncrementalHFiles</span> <span class="variable">loder</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LoadIncrementalHFiles</span>(configuration);</span><br><span class="line">loder.doBulkLoad(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;hdfs://output&quot;</span>),<span class="keyword">new</span> <span class="title class_">HTable</span>(conf,<span class="string">&quot;table_name&quot;</span>));</span><br></pre></td></tr></table></figure>

<h2 id="使用spark进行Bulkload"><a href="#使用spark进行Bulkload" class="headerlink" title="使用spark进行Bulkload"></a>使用spark进行Bulkload</h2><p>在前面生成HFile的步骤中，使用了MR生成，效率较慢，且不能很好的使用hive特性，只能从底层读取hdfs文件进行解析，对于不同格式的数据文件需要进行不同操作。而通过使用spark on hive可以很好的借用hive特性，屏蔽底层数据文件格式，且使用spark也在执行效率上进行提升。<br>和MR类似，使用spark也是通过把数据转换成(ImmutableBytesWritable, Put)对，或者(ImmutableBytesWritable, KeyValue)对，示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> partitionData = data.rdd.map(s =&gt; &#123;</span><br><span class="line">      ((s.get(<span class="number">0</span>).toString, s.get(<span class="number">1</span>).toString, s.getString(<span class="number">2</span>)), s.getString(<span class="number">3</span>))</span><br><span class="line">    &#125;).repartitionAndSortWithinPartitions(<span class="keyword">new</span> <span class="type">SaltPrefixPartitioner</span>(hashV))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">val</span> hfile = partitionData.map &#123; s =&gt;</span><br><span class="line">      <span class="keyword">val</span> uid = s._1._1</span><br><span class="line">      <span class="keyword">val</span> rowkey = uid</span><br><span class="line">      <span class="keyword">val</span> immutable = <span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>(<span class="type">Bytes</span>.toBytes(rowkey))</span><br><span class="line">      <span class="keyword">val</span> value = s._2</span><br><span class="line">      <span class="keyword">val</span> keyValue = <span class="keyword">new</span> <span class="type">KeyValue</span>(<span class="type">Bytes</span>.toBytes(rowkey), <span class="type">Bytes</span>.toBytes(s._1._2),</span><br><span class="line">        <span class="type">Bytes</span>.toBytes(s._1._3), <span class="type">Utils</span>.transferDateToTs(dt), <span class="type">Bytes</span>.toBytes(value))</span><br><span class="line">      (immutable, keyValue)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>在示例中，data是个DataFrame，schema为：用户id,列族，列名，value。但是我们在查询hive数据时往往表结构为用户id，列1，列2 …. ，所以需要把其转成hbase存储方式（rowkey，family,qualifier,value）。其中有个很重要的一步：repartitionAndSortWithinPartitions。在MR时，MR框架会进行SortReducer，所以刚好满足了HFile有序的要求，但是spark计算生成HFile的过程中只有map类算子，是无序的，所以需要手工进行排序操作。<strong>在Hbase中，需要按照(rowkey，family,qualifier)顺序进行排序</strong>，因为我在Hbase表进行了手工region划分，而region可正好对应spark partition，所以就很好的利用repartitionAndSortWithinPartitions这个算子进行region划分并对region中的数据进行排序了。SaltPrefixPartitioner是我实现的根据用户id进行hash分区的算法。</p>
<p>然后仍然需要定义个Job,和BulkLoadDriver中一样，指定MapOutputKeyClass，MapOutputValueClass，OutputFormatClass，然后使用HFileOutputFormat2.configureIncrementalLoad(job,table,table.getRegionLocator()) ，再调用rdd的算子saveAsNewAPIHadoopFile。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hfile.saveAsNewAPIHadoopFile(stagingFolder, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], job.getConfiguration)</span><br></pre></td></tr></table></figure>
<p>这样就按照HFile格式把数据生成在了stagingFolder目录中，最后还需执行Bulkload步骤。</p>
<h2 id="spark-Bulkload的一些注意事项"><a href="#spark-Bulkload的一些注意事项" class="headerlink" title="spark Bulkload的一些注意事项"></a>spark Bulkload的一些注意事项</h2><h3 id="kerberos权限问题"><a href="#kerberos权限问题" class="headerlink" title="kerberos权限问题"></a>kerberos权限问题</h3><p>在整个过程中，其实是使用了两套账号，在提交spark的过程中，是spark的账号管理，且会提交给yarn(所以账号权限是客户端的账号权限)；而使用MR的话，整个job是通过程序中的configuration配置的，所以你可以对其进行任何权限配置。在Bulkload过程中，也是使用配程序configuration，也是可随意账号配置。而Bulkload只是个普通hdfs操作，并没有通过yarn（也在调研能否通过yarn模式）。</p>
<h3 id="数据snappy压缩问题"><a href="#数据snappy压缩问题" class="headerlink" title="数据snappy压缩问题"></a>数据snappy压缩问题</h3><p>有时候我们需要把HFile进行压缩，以减少文件存储，在这里我使用了snappy压缩（snappy在存储和CPU计算上相对其他压缩算法更平衡）。因为snappy压缩依赖snappy.so本地方法库，在进行spark计算生成HFile或Bulkload过程中都可能出现这样的错误：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: native snappy library not available: this version of libhadoop was built without snappy support</span><br></pre></td></tr></table></figure>
<p>如果在spark计算生成HFile过程中报错，这就涉及了spark配置读取的问题（因为我们集群对snappy压缩是没问题的，这肯定是在spark任务提交过程中读取了错误的配置项），后续我会专门讲解spark的配置加载。因为我们spark客户端（docker镜像）是官网spark版本，而集群是使用的CDH部署的，所以在配置上有所不同，以及spark文件路径也有所不同。所以问题也出于此，最终在spark-default.conf加上即可。</p>
<p>而在Bulkload过程中，通过LoadIncrementalHFiles源码我们可以知道，因为会根据region进行加载（groupOrSplitPhase），他会去读取HFile文件的位置偏移，如果当前执行Bulkload客户端的hadoop没有snappy库或者配置错误的话，也就读取HFile失败。</p>
<h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>HBase 官方和cloudera都提供了hbase-spark模块，里面有HbaseContext类，很好的封装了spark在Hbase上面的使用（包括读取Hbase数据转成rdd，Bulkload等），但cloudera版本的使用的spark依然为1.6，所以在使用过程中需要依赖的包也是1.6，而我们docker环境的spark版本是2.0，会有一些问题（看了下code，竟然引入了spark某个不重要的Logging类，然鹅在2.0这个类被移了位置 囧），后续也没有更新了。<br>Apache hbase-spark_2.0我看源码好像修复了，不在依赖具体spark中的logging类了，但我们CDH中的hbase版本降低，担心会有其他影响，便还未进行测试。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">weibingo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wbice.cn/article/spark-bulkload.html">https://wbice.cn/article/spark-bulkload.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/spark/">spark</a><a class="post-meta__tags" href="/tags/hbase/">hbase</a></div><div class="addthis_inline_share_toolbox pull-right"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=undefined" async></script><nav id="pagination"><div class="prev-post pull-left"><a href="/article/American-drama.html"><i class="fa fa-chevron-left">  </i><span>打算观看的美剧</span></a></div><div class="next-post pull-right"><a href="/article/economic-thinking-two.html"><span>经济思维读书笔记（二）</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == 'true';
var verify = 'false' == 'true';
var record_ip = '' == 'true';
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  recordIP:record_ip,
  appId:'dEfDWKbVoLq95IOjY2Wucz01-9Nh9j0Va',
  appKey:'CWRYxwiLTYcAG0OcWmKl7Ez0',
  placeholder:'ヾﾉ≧∀≦)o来啊，快活啊，造作啊!',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(https://hexo-1256892004.cos.ap-beijing.myqcloud.com/page/top.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2023 By weibingo</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.8.0"></script><script src="/js/fancybox.js?version=1.8.0"></script><script src="/js/sidebar.js?version=1.8.0"></script><script src="/js/copy.js?version=1.8.0"></script><script src="/js/fireworks.js?version=1.8.0"></script><script src="/js/transition.js?version=1.8.0"></script><script src="/js/scroll.js?version=1.8.0"></script><script src="/js/head.js?version=1.8.0"></script><script src="/js/search/algolia.js"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>